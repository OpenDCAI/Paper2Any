import os
import asyncio
import logging
from typing import Optional
import gradio as gr
from langgraph.graph import StateGraph, START, END

from dataflow_agent.state import DataCollectionRequest, DataCollectionState
from dataflow_agent.agentroles.dataconvertor import universal_data_conversion
from script.run_dfa_web_collection import web_crawl_collection


def create_web_collection():
    """å­é¡µé¢ï¼šç½‘é¡µæ•°æ®é‡‡é›†ä¸è½¬æ¢ï¼ˆåŸºäº run_web_pipeline å·¥ä½œæµï¼‰"""
    with gr.Blocks() as page:
        gr.Markdown("# ğŸŒ ç½‘é¡µæ•°æ®é‡‡é›†ä¸è½¬æ¢")

        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
            with gr.Column():
                gr.Markdown("### é‡‡é›†é…ç½®")
                target = gr.Textbox(
                    label="ç›®æ ‡æè¿°",
                    placeholder="ä¾‹å¦‚ï¼šæ”¶é›† Python ä»£ç ç¤ºä¾‹çš„æ•°æ®é›†",
                    lines=3
                )
                category = gr.Dropdown(
                    label="æ•°æ®ç±»åˆ«",
                    choices=["PT", "SFT"],
                    value="SFT"
                )
                dataset_num_limit = gr.Slider(
                    label="æ•°æ®é›†æ•°é‡ä¸Šé™ï¼ˆæ¯å…³é”®è¯ï¼Œä»…ç”¨äºå‚è€ƒï¼‰",
                    minimum=1,
                    maximum=50,
                    step=1,
                    value=5
                )
                dataset_size_category = gr.Dropdown(
                    label="æ•°æ®é›†å¤§å°èŒƒå›´",
                    choices=["n<1K", "1K<n<10K", "10K<n<100K", "100K<n<1M", "n>1M"],
                    value="1K<n<10K"
                )
                max_download_subtasks = gr.Number(
                    label="ä¸‹è½½å­ä»»åŠ¡ä¸Šé™",
                    value=None,
                    precision=0,
                    minimum=0,
                    info="é™åˆ¶æœ€ç»ˆæ‰§è¡Œçš„ä¸‹è½½å­ä»»åŠ¡æ•°é‡ï¼Œç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶"
                )
                with gr.Row():
                    max_dataset_size_value = gr.Number(
                        label="æœ€å¤§æ•°æ®é›†å¤§å°",
                        value=None,
                        precision=0,
                        minimum=0,
                        info="å¯ç•™ç©ºï¼›è¾“å…¥æ•°å€¼åé€‰æ‹©å•ä½"
                    )
                    max_dataset_size_unit = gr.Dropdown(
                        label="å•ä½",
                        choices=["B", "KB", "MB", "GB", "TB"],
                        value="GB"
                    )
                download_dir = gr.Textbox(
                    label="ä¸‹è½½ç›®å½•",
                    value="downloaded_data",
                )
                language = gr.Dropdown(
                    label="æç¤ºè¯è¯­è¨€",
                    choices=["zh", "en"],
                    value="zh"
                )

                gr.Markdown("### LLM é…ç½®")
                chat_api_url = gr.Textbox(
                    label="CHAT_API_URL",
                    value=os.getenv("CHAT_API_URL", "http://123.129.219.111:3000/v1/chat/completions")
                )
                api_key = gr.Textbox(
                    label="CHAT_API_KEY",
                    value=os.getenv("CHAT_API_KEY", ""),
                    type="password"
                )
                model = gr.Textbox(
                    label="CHAT_MODEL",
                    value=os.getenv("CHAT_MODEL", "deepseek-chat")
                )

                gr.Markdown("### å…¶ä»–ç¯å¢ƒé…ç½®")
                hf_endpoint = gr.Textbox(
                    label="HF_ENDPOINT",
                    value=os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
                )
                kaggle_username = gr.Textbox(
                    label="KAGGLE_USERNAME",
                    value=os.getenv("KAGGLE_USERNAME", "")
                )
                kaggle_key = gr.Textbox(
                    label="KAGGLE_KEY",
                    value=os.getenv("KAGGLE_KEY", ""),
                    type="password"
                )
                tavily_api_key = gr.Textbox(
                    label="TAVILY_API_KEY",
                    value=os.getenv("TAVILY_API_KEY", ""),
                    type="password"
                )

                gr.Markdown("### RAG é…ç½®")
                rag_ebd_model = gr.Textbox(
                    label="RAG_EBD_MODEL",
                    value=os.getenv("RAG_EBD_MODEL", "text-embedding-3-large")
                )
                rag_api_url = gr.Textbox(
                    label="RAG_API_URL",
                    value=os.getenv("RAG_API_URL", "http://123.129.219.111:3000/v1/chat/completions")
                )
                rag_api_key = gr.Textbox(
                    label="RAG_API_KEY",
                    value=os.getenv("RAG_API_KEY", ""),
                    type="password"
                )

                # é«˜çº§é…ç½®åŒºåŸŸï¼ˆå¯æŠ˜å ï¼‰
                with gr.Accordion("âš™ï¸ é«˜çº§é…ç½®", open=False):
                    gr.Markdown("### ç½‘é¡µé‡‡é›†é«˜çº§é…ç½®")
                    max_crawl_cycles_per_task = gr.Slider(
                        label="ä¸‹è½½ä»»åŠ¡æœ€å¤§å¾ªç¯æ¬¡æ•°",
                        minimum=1,
                        maximum=50,
                        step=1,
                        value=10,
                        info="æ§åˆ¶æ¯ä¸ªä¸‹è½½ä»»åŠ¡çš„æœ€å¤§é‡è¯•å¾ªç¯æ¬¡æ•°"
                    )
                    max_crawl_cycles_for_research = gr.Slider(
                        label="ç ”ç©¶é˜¶æ®µæœ€å¤§å¾ªç¯æ¬¡æ•°",
                        minimum=1,
                        maximum=50,
                        step=1,
                        value=15,
                        info="researché˜¶æ®µçš„æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œå…è®¸è®¿é—®æ›´å¤šç½‘ç«™"
                    )
                    search_engine = gr.Dropdown(
                        label="æœç´¢å¼•æ“",
                        choices=["tavily", "duckduckgo", "jina"],
                        value="tavily",
                        info="é€‰æ‹©ç”¨äºæœç´¢çš„å¼•æ“"
                    )
                    use_jina_reader = gr.Checkbox(
                        label="ä½¿ç”¨ Jina Reader",
                        value=True,
                        info="æ˜¯å¦ä½¿ç”¨ Jina Reader æå–ç½‘é¡µç»“æ„åŒ–å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼Œå¿«é€Ÿï¼‰"
                    )
                    enable_rag = gr.Checkbox(
                        label="å¯ç”¨ RAG å¢å¼º",
                        value=True,
                        info="æ˜¯å¦å¯ç”¨ RAG å¢å¼ºï¼ˆæ— è®ºä½¿ç”¨å“ªç§è§£ææ–¹æ³•ï¼Œéƒ½ç”¨ RAG ç²¾ç‚¼å†…å®¹ï¼‰"
                    )
                    concurrent_pages = gr.Slider(
                        label="å¹¶è¡Œå¤„ç†é¡µé¢æ•°",
                        minimum=1,
                        maximum=20,
                        step=1,
                        value=5,
                        info="å¹¶è¡Œå¤„ç†çš„é¡µé¢æ•°é‡ï¼Œå¯æ ¹æ®ç½‘ç»œå’Œæœºå™¨æ€§èƒ½è°ƒæ•´ï¼ˆå»ºè®®3-10ï¼‰"
                    )
                    disable_cache = gr.Checkbox(
                        label="ç¦ç”¨ç¼“å­˜",
                        value=True,
                        info="å¦‚æœå¯ç”¨ï¼Œå°†å®Œå…¨ç¦ç”¨ HuggingFace å’Œ Kaggle çš„ç¼“å­˜ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•å¹¶åœ¨ä¸‹è½½åè‡ªåŠ¨æ¸…ç†"
                    )
                    temp_base_dir = gr.Textbox(
                        label="ä¸´æ—¶ç›®å½•ï¼ˆå¯é€‰ï¼‰",
                        value="",
                        placeholder="ç•™ç©ºåˆ™ä½¿ç”¨é»˜è®¤ä¸´æ—¶ç›®å½•",
                        info="è‡ªå®šä¹‰ä¸´æ—¶ç›®å½•è·¯å¾„ï¼Œç”¨äºç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶"
                    )

                    gr.Markdown("### æ•°æ®è½¬æ¢é«˜çº§é…ç½®")
                    conversion_temperature = gr.Slider(
                        label="è½¬æ¢æ¨¡å‹æ¸©åº¦",
                        minimum=0.0,
                        maximum=2.0,
                        step=0.1,
                        value=0.0,
                        info="æ•°æ®è½¬æ¢æ—¶ä½¿ç”¨çš„æ¨¡å‹æ¸©åº¦å‚æ•°"
                    )
                    conversion_max_tokens = gr.Slider(
                        label="è½¬æ¢æœ€å¤§ Token æ•°",
                        minimum=512,
                        maximum=8192,
                        step=256,
                        value=4096,
                        info="æ•°æ®è½¬æ¢æ—¶çš„æœ€å¤§ token æ•°"
                    )
                    conversion_max_sample_length = gr.Slider(
                        label="æœ€å¤§é‡‡æ ·é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰",
                        minimum=50,
                        maximum=1000,
                        step=50,
                        value=200,
                        info="æ¯ä¸ªå­—æ®µçš„æœ€å¤§é‡‡æ ·é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰"
                    )
                    conversion_num_sample_records = gr.Slider(
                        label="é‡‡æ ·è®°å½•æ•°é‡",
                        minimum=1,
                        maximum=10,
                        step=1,
                        value=3,
                        info="ç”¨äºåˆ†æçš„é‡‡æ ·è®°å½•æ•°é‡"
                    )

                submit_btn = gr.Button("å¼€å§‹ç½‘é¡µé‡‡é›†ä¸è½¬æ¢", variant="primary")

            # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
            with gr.Column():
                with gr.Tab("æ‰§è¡Œæ—¥å¿—"):
                    output_log = gr.Textbox(label="æ—¥å¿—", lines=18)
                with gr.Tab("ç»“æœæ‘˜è¦"):
                    output_json = gr.JSON(label="æ‰§è¡Œç»“æœ")

        async def run_pipeline(
            target_text: str,
            category_val: str,
            dataset_num_limit_val: int,
            dataset_size_category_val: str,
            max_download_subtasks_val: float | None,
            max_dataset_size_value_val: float | None,
            max_dataset_size_unit_val: str,
            download_dir_val: str,
            language_val: str,
            chat_api_url_val: str,
            api_key_val: str,
            model_val: str,
            hf_endpoint_val: str,
            kaggle_username_val: str,
            kaggle_key_val: str,
            rag_ebd_model_val: str,
            rag_api_url_val: str,
            rag_api_key_val: str,
            tavily_api_key_val: str,
            # é«˜çº§é…ç½®å‚æ•°
            max_crawl_cycles_per_task_val: int,
            max_crawl_cycles_for_research_val: int,
            search_engine_val: str,
            use_jina_reader_val: bool,
            enable_rag_val: bool,
            concurrent_pages_val: int,
            disable_cache_val: bool,
            temp_base_dir_val: str,
            conversion_temperature_val: float,
            conversion_max_tokens_val: int,
            conversion_max_sample_length_val: int,
            conversion_num_sample_records_val: int,
        ):
            # æ³¨å…¥/è¦†ç›–è¿è¡Œæ‰€éœ€çš„ç¯å¢ƒå˜é‡
            os.environ["CHAT_API_URL"] = chat_api_url_val or ""
            os.environ["CHAT_API_KEY"] = api_key_val or ""
            os.environ["CHAT_MODEL"] = model_val or ""
            os.environ["HF_ENDPOINT"] = hf_endpoint_val or ""
            os.environ["KAGGLE_USERNAME"] = kaggle_username_val or ""
            os.environ["KAGGLE_KEY"] = kaggle_key_val or ""
            os.environ["RAG_EBD_MODEL"] = rag_ebd_model_val or ""
            os.environ["RAG_API_URL"] = rag_api_url_val or ""
            os.environ["RAG_API_KEY"] = rag_api_key_val or ""
            if tavily_api_key_val:
                os.environ["TAVILY_API_KEY"] = tavily_api_key_val
            else:
                os.environ.pop("TAVILY_API_KEY", None)

            # è®¾ç½®é«˜çº§é…ç½®ç›¸å…³ç¯å¢ƒå˜é‡
            if disable_cache_val:
                os.environ["DF_DISABLE_CACHE"] = "true"
            else:
                os.environ.pop("DF_DISABLE_CACHE", None)

            if temp_base_dir_val:
                os.environ["DF_TEMP_DIR"] = temp_base_dir_val
            else:
                os.environ.pop("DF_TEMP_DIR", None)

            # ç»„è£…è¯·æ±‚
            def _convert_size_to_bytes(value: float | None, unit: str) -> Optional[int]:
                if value is None:
                    return None
                try:
                    numeric = float(value)
                except (TypeError, ValueError):
                    return None
                if numeric <= 0:
                    return None
                unit = (unit or "B").upper()
                multipliers = {
                    "B": 1,
                    "KB": 1024,
                    "MB": 1024 ** 2,
                    "GB": 1024 ** 3,
                    "TB": 1024 ** 4,
                }
                multiplier = multipliers.get(unit, 1)
                return int(numeric * multiplier)

            max_dataset_size_bytes = _convert_size_to_bytes(max_dataset_size_value_val, max_dataset_size_unit_val)

            def _normalize_download_limit(value: float | None) -> Optional[int]:
                if value is None:
                    return None
                try:
                    numeric = int(value)
                except (TypeError, ValueError):
                    return None
                if numeric <= 0:
                    return None
                return numeric

            max_download_subtasks_int = _normalize_download_limit(max_download_subtasks_val)

            req = DataCollectionRequest(
                target=target_text,
                category=category_val,
                dataset_num_limit=int(dataset_num_limit_val),
                dataset_size_category=dataset_size_category_val,
                max_dataset_size=max_dataset_size_bytes,
                max_download_subtasks=max_download_subtasks_int,
                download_dir=download_dir_val,
                chat_api_url=chat_api_url_val,
                api_key=api_key_val,
                model=model_val,
                language=language_val,
                tavily_api_key=tavily_api_key_val or None,
            )

            # æ„å»ºå·¥ä½œæµ
            state = DataCollectionState(request=req)

            # åˆ›å»ºåŒ…è£…å‡½æ•°ä»¥ä¼ é€’é«˜çº§é…ç½®å‚æ•°
            async def web_crawl_collection_wrapper(state: DataCollectionState) -> DataCollectionState:
                return await web_crawl_collection(
                    state,
                    max_crawl_cycles_per_task=int(max_crawl_cycles_per_task_val),
                    max_crawl_cycles_for_research=int(max_crawl_cycles_for_research_val),
                    search_engine=search_engine_val,
                    use_jina_reader=use_jina_reader_val,
                    enable_rag=enable_rag_val,
                    concurrent_pages=int(concurrent_pages_val),
                    disable_cache=bool(disable_cache_val),
                    temp_base_dir=(temp_base_dir_val.strip() or None) if isinstance(temp_base_dir_val, str) else None,
                    max_download_subtasks=max_download_subtasks_int,
                )

            async def universal_data_conversion_wrapper(state: DataCollectionState) -> DataCollectionState:
                return await universal_data_conversion(
                    state,
                    model_name=model_val or None,
                    temperature=float(conversion_temperature_val),
                    max_tokens=int(conversion_max_tokens_val),
                    max_sample_length=int(conversion_max_sample_length_val),
                    num_sample_records=int(conversion_num_sample_records_val),
                )

            graph_builder = StateGraph(DataCollectionState)
            graph_builder.add_node("web_crawl_collection", web_crawl_collection_wrapper)
            graph_builder.add_node("universal_data_conversion", universal_data_conversion_wrapper)
            graph_builder.add_edge(START, "web_crawl_collection")
            graph_builder.add_edge("web_crawl_collection", "universal_data_conversion")
            graph_builder.add_edge("universal_data_conversion", END)
            graph = graph_builder.compile()

            header_lines = [
                "=" * 60,
                "å¼€å§‹æ‰§è¡Œç½‘é¡µé‡‡é›†ä¸è½¬æ¢å·¥ä½œæµ",
                "=" * 60,
                f"ç›®æ ‡: {req.target}",
                f"ç±»åˆ«: {req.category}",
                f"ä¸‹è½½ç›®å½•: {req.download_dir}",
                "\nã€ç½‘é¡µé‡‡é›†é…ç½®ã€‘",
                f"  - æœç´¢å¼•æ“: {search_engine_val}",
                f"  - ä¸‹è½½å­ä»»åŠ¡ä¸Šé™: {max_download_subtasks_int if max_download_subtasks_int is not None else 'ä¸é™åˆ¶'}",
                f"  - ä»»åŠ¡æœ€å¤§å¾ªç¯æ¬¡æ•°: {max_crawl_cycles_per_task_val}",
                f"  - ç ”ç©¶é˜¶æ®µæœ€å¤§å¾ªç¯æ¬¡æ•°: {max_crawl_cycles_for_research_val}",
                f"  - ä½¿ç”¨ Jina Reader: {'æ˜¯' if use_jina_reader_val else 'å¦'}",
                f"  - å¯ç”¨ RAG: {'æ˜¯' if enable_rag_val else 'å¦'}",
                f"  - å¹¶è¡Œé¡µé¢æ•°: {concurrent_pages_val}",
                f"  - ç¦ç”¨ç¼“å­˜: {'æ˜¯' if disable_cache_val else 'å¦'}",
                "\nã€æ•°æ®è½¬æ¢é…ç½®ã€‘",
                f"  - æ¨¡å‹æ¸©åº¦: {conversion_temperature_val}",
                f"  - æœ€å¤§ Token æ•°: {conversion_max_tokens_val}",
                f"  - æœ€å¤§é‡‡æ ·é•¿åº¦: {conversion_max_sample_length_val}",
                f"  - é‡‡æ ·è®°å½•æ•°: {conversion_num_sample_records_val}",
                f"\næ•°æ®é›†å¤§å°é™åˆ¶: {max_dataset_size_bytes if max_dataset_size_bytes else 'ä¸é™åˆ¶'}",
                "=" * 60,
            ]

            log_lines: list[str] = header_lines.copy()
            log_queue: asyncio.Queue = asyncio.Queue()

            class DataflowLogFilter(logging.Filter):
                def filter(self, record: logging.LogRecord) -> bool:  # type: ignore[override]
                    return record.name.startswith("dataflow_agent") or record.name.startswith("script")

            class GradioLogHandler(logging.Handler):
                def __init__(self, queue: asyncio.Queue[str]):
                    super().__init__(level=logging.INFO)
                    self.queue = queue
                    self.addFilter(DataflowLogFilter())
                    self.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))

                def emit(self, record: logging.LogRecord) -> None:  # type: ignore[override]
                    try:
                        message = self.format(record)
                        loop = asyncio.get_running_loop()
                        loop.call_soon_threadsafe(self.queue.put_nowait, message)
                    except RuntimeError:
                        try:
                            self.queue.put_nowait(self.format(record))
                        except asyncio.QueueFull:
                            pass
                    except Exception:
                        pass

            handler = GradioLogHandler(log_queue)
            root_logger = logging.getLogger()
            root_logger.addHandler(handler)
            original_level = root_logger.level
            if original_level == 0 or original_level > logging.INFO:
                root_logger.setLevel(logging.INFO)

            attached_loggers: set[logging.Logger] = {root_logger}

            def _attach_to_existing_loggers() -> None:
                logger_dict = logging.root.manager.loggerDict  # type: ignore[attr-defined]
                for name in list(logger_dict.keys()):
                    if isinstance(name, str) and (name.startswith("dataflow_agent") or name.startswith("script")):
                        logger_obj = logging.getLogger(name)
                        logger_obj.addHandler(handler)
                        attached_loggers.add(logger_obj)

                for name in ("dataflow_agent", "script"):
                    logger_obj = logging.getLogger(name)
                    logger_obj.addHandler(handler)
                    attached_loggers.add(logger_obj)

            _attach_to_existing_loggers()

            # åˆå§‹è¾“å‡º
            yield "\n".join(log_lines), gr.update(value=None)

            async def run_workflow() -> DataCollectionState:
                return await graph.ainvoke(state)

            workflow_task = asyncio.create_task(run_workflow())
            result_payload: Optional[dict] = None

            try:
                while True:
                    try:
                        message = await asyncio.wait_for(log_queue.get(), timeout=0.3)
                        log_lines.append(message)
                        yield "\n".join(log_lines), gr.update(value=result_payload)
                    except asyncio.TimeoutError:
                        if workflow_task.done():
                            break

                await workflow_task

                # æ¸…ç©ºå‰©ä½™æ—¥å¿—
                while True:
                    try:
                        pending = log_queue.get_nowait()
                        log_lines.append(pending)
                    except asyncio.QueueEmpty:
                        break

                log_lines.append("æµç¨‹æ‰§è¡Œå®Œæˆï¼")

                result_payload = {
                    "download_dir": req.download_dir,
                    "processed_output": os.path.join(req.download_dir, "processed_output"),
                    "category": req.category,
                    "language": req.language,
                    "chat_model": req.model,
                    "max_download_subtasks": req.max_download_subtasks,
                    "max_dataset_size_bytes": req.max_dataset_size,
                    "max_dataset_size_unit": max_dataset_size_unit_val if req.max_dataset_size else None,
                    "max_dataset_size_value": max_dataset_size_value_val if req.max_dataset_size else None,
                }

                yield "\n".join(log_lines), result_payload

            except Exception as exc:
                error_message = f"æµç¨‹æ‰§è¡Œå¤±è´¥: {exc}"
                log_lines.append(error_message)
                while True:
                    try:
                        pending = log_queue.get_nowait()
                        log_lines.append(pending)
                    except asyncio.QueueEmpty:
                        break
                result_payload = {"error": str(exc)}
                yield "\n".join(log_lines), result_payload
                raise
            finally:
                for logger_obj in attached_loggers:
                    logger_obj.removeHandler(handler)
                handler.close()
                root_logger.setLevel(original_level)

        submit_btn.click(
            run_pipeline,
            inputs=[
                target,
                category,
                dataset_num_limit,
                dataset_size_category,
                max_download_subtasks,
                max_dataset_size_value,
                max_dataset_size_unit,
                download_dir,
                language,
                chat_api_url,
                api_key,
                model,
                hf_endpoint,
                kaggle_username,
                kaggle_key,
                rag_ebd_model,
                rag_api_url,
                rag_api_key,
                tavily_api_key,
                # é«˜çº§é…ç½®å‚æ•°
                max_crawl_cycles_per_task,
                max_crawl_cycles_for_research,
                search_engine,
                use_jina_reader,
                enable_rag,
                concurrent_pages,
                disable_cache,
                temp_base_dir,
                conversion_temperature,
                conversion_max_tokens,
                conversion_max_sample_length,
                conversion_num_sample_records,
            ],
            outputs=[output_log, output_json],
        )

    return page


