"""
memory_leak_test.py
ç‹¬ç«‹çš„å†…å­˜æ³„æ¼æ£€æµ‹è„šæœ¬ - æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
"""

import asyncio
import gc
import json
import os
import sys
import tempfile
import tracemalloc
from pathlib import Path
from typing import List, Dict, Any, Set
import psutil
import re
import pandas as pd
import uuid
import textwrap


# ============================================================================
#                          å¤åˆ¶éœ€è¦æµ‹è¯•çš„å‡½æ•°
# ============================================================================

def _patch_first_entry_file(py_file: str | Path,
                            old_path: str,
                            new_path: str) -> None:
    """ä»åŸä»£ç å¤åˆ¶"""
    py_file = Path(py_file).expanduser().resolve()
    code = py_file.read_text(encoding="utf-8")

    pattern = (
        r'first_entry_file_name\s*=\s*[\'"]'
        + re.escape(old_path)
        + r'[\'"]'
    )
    replacement = f'first_entry_file_name=\"{new_path}\"'
    new_code, n = re.subn(pattern, replacement, code, count=1)
    if n == 0:
        new_code = code.replace(old_path, new_path)

    py_file.write_text(new_code, encoding="utf-8")


def _ensure_py_file(code: str, file_name: str | None = None) -> Path:
    """ä»åŸä»£ç å¤åˆ¶"""
    if file_name:
        target = Path(file_name).expanduser().resolve()
    else:
        target = Path(tempfile.gettempdir()) / f"recommend_pipeline_{uuid.uuid4().hex}.py"
    target.write_text(textwrap.dedent(code), encoding="utf-8")
    print(f"[Test] pipeline code written to {target}")
    return target


def _create_debug_sample(src_file: str | Path, sample_lines: int = 10) -> Path:
    """ä»åŸä»£ç å¤åˆ¶"""
    src_path = Path(src_file).expanduser().resolve()
    if not src_path.is_file():
        raise FileNotFoundError(f"source file not found: {src_path}")

    tmp_path = (
        Path(tempfile.gettempdir())
        / f"{src_path.stem}_sample_{sample_lines}{src_path.suffix}"
    )

    suffix = src_path.suffix.lower()
    
    if suffix == '.csv':
        df = pd.read_csv(src_path)
        sample_df = df.head(sample_lines)
        sample_df.to_csv(tmp_path, index=False, encoding="utf-8")
        print(f"[Test] CSV sample written to {tmp_path}")
    
    elif suffix == '.json':
        with src_path.open("r", encoding="utf-8") as f:
            first_char = f.read(1)
            f.seek(0)
            
            if first_char == '[':
                data = json.load(f)
                sample_data = data[:sample_lines]
                with tmp_path.open("w", encoding="utf-8") as wf:
                    json.dump(sample_data, wf, ensure_ascii=False, indent=2)
            else:
                sample_data = []
                for idx, line in enumerate(f):
                    if idx >= sample_lines:
                        break
                    sample_data.append(json.loads(line.strip()))
                
                with tmp_path.open("w", encoding="utf-8") as wf:
                    for item in sample_data:
                        wf.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        print(f"[Test] JSON sample written to {tmp_path}")
    
    elif suffix == '.jsonl':
        sample_data = []
        with src_path.open("r", encoding="utf-8") as f:
            for idx, line in enumerate(f):
                if idx >= sample_lines:
                    break
                sample_data.append(json.loads(line.strip()))
        
        with tmp_path.open("w", encoding="utf-8") as wf:
            for item in sample_data:
                wf.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        print(f"[Test] JSONL sample written to {tmp_path}")
    
    else:
        raise ValueError(f"Unsupported file format: {suffix}")

    return tmp_path


async def _run_py(file_path: Path) -> dict:
    """ä»åŸä»£ç å¤åˆ¶"""
    proc = await asyncio.create_subprocess_exec(
        sys.executable, str(file_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout_b, stderr_b = await proc.communicate()
    stdout, stderr = stdout_b.decode(), stderr_b.decode()

    return {
        "success": proc.returncode == 0,
        "return_code": proc.returncode,
        "stdout": stdout,
        "stderr": stderr,
        "file_path": str(file_path),
    }


# ============================================================================
#                          å†…å­˜æ³„æ¼æ£€æµ‹å™¨
# ============================================================================

class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.snapshots = []
        self.temp_files_created = []
        
    def start_tracking(self):
        """å¼€å§‹è¿½è¸ªå†…å­˜"""
        gc.collect()
        tracemalloc.start()
        self.baseline_memory = self.process.memory_info().rss / 1024 / 1024
        print(f"[MemoryCheck] åŸºçº¿å†…å­˜: {self.baseline_memory:.2f} MB")
        
    def take_snapshot(self, label: str):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        gc.collect()
        current_memory = self.process.memory_info().rss / 1024 / 1024
        snapshot = tracemalloc.take_snapshot()
        
        self.snapshots.append({
            'label': label,
            'memory_mb': current_memory,
            'memory_delta': current_memory - self.baseline_memory,
            'snapshot': snapshot
        })
        
        print(f"[MemoryCheck] {label}: {current_memory:.2f} MB "
              f"(+{current_memory - self.baseline_memory:.2f} MB)")
        
    def track_temp_file(self, file_path: Path):
        """è®°å½•åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶"""
        self.temp_files_created.append(file_path)
        
    def check_temp_file_cleanup(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶æ˜¯å¦è¢«æ¸…ç†"""
        uncleaned = []
        total_size = 0
        
        for fp in self.temp_files_created:
            if fp.exists():
                size = fp.stat().st_size
                uncleaned.append({
                    'path': str(fp),
                    'size_kb': size / 1024
                })
                total_size += size
                
        return {
            'uncleaned_count': len(uncleaned),
            'total_size_mb': total_size / 1024 / 1024,
            'files': uncleaned
        }
        
    def compare_snapshots(self, index1: int, index2: int, top_n: int = 10):
        """æ¯”è¾ƒä¸¤ä¸ªå¿«ç…§"""
        if index1 >= len(self.snapshots) or index2 >= len(self.snapshots):
            return
            
        snap1 = self.snapshots[index1]['snapshot']
        snap2 = self.snapshots[index2]['snapshot']
        
        stats = snap2.compare_to(snap1, 'lineno')
        
        print(f"\n[MemoryCheck] Top {top_n} å†…å­˜å¢é•¿:")
        print(f"ä» '{self.snapshots[index1]['label']}' åˆ° '{self.snapshots[index2]['label']}'")
        print("-" * 80)
        
        for stat in stats[:top_n]:
            print(f"{stat}")
            
    def stop_tracking(self):
        """åœæ­¢è¿½è¸ª"""
        tracemalloc.stop()
        
    def generate_report(self) -> str:
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        report = ["=" * 80]
        report.append("å†…å­˜æ³„æ¼æ£€æµ‹æŠ¥å‘Š")
        report.append("=" * 80)
        
        report.append("\n1. å†…å­˜å¢é•¿è¶‹åŠ¿:")
        for snap in self.snapshots:
            report.append(f"  {snap['label']}: {snap['memory_mb']:.2f} MB "
                         f"(+{snap['memory_delta']:.2f} MB)")
            
        cleanup_status = self.check_temp_file_cleanup()
        report.append(f"\n2. ä¸´æ—¶æ–‡ä»¶æ¸…ç†:")
        report.append(f"  åˆ›å»ºæ–‡ä»¶æ€»æ•°: {len(self.temp_files_created)}")
        report.append(f"  æœªæ¸…ç†æ–‡ä»¶æ•°: {cleanup_status['uncleaned_count']}")
        report.append(f"  å ç”¨ç©ºé—´: {cleanup_status['total_size_mb']:.2f} MB")
        
        if cleanup_status['files']:
            report.append("  æœªæ¸…ç†æ–‡ä»¶åˆ—è¡¨ (å‰10ä¸ª):")
            for f in cleanup_status['files'][:10]:
                report.append(f"    - {f['path']} ({f['size_kb']:.2f} KB)")
                
        report.append("\n3. æ³„æ¼é£é™©è¯„ä¼°:")
        total_growth = self.snapshots[-1]['memory_delta'] if self.snapshots else 0
        
        risk_items = []
        if total_growth > 100:
            risk_items.append("âš ï¸  å†…å­˜å¢é•¿è¶…è¿‡100MB")
        elif total_growth > 50:
            risk_items.append("âš ï¸  å†…å­˜å¢é•¿è¶…è¿‡50MB")
            
        if cleanup_status['uncleaned_count'] > 10:
            risk_items.append(f"âš ï¸  æœ‰ {cleanup_status['uncleaned_count']} ä¸ªä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†")
        elif cleanup_status['uncleaned_count'] > 0:
            risk_items.append(f"âš ï¸  æœ‰ {cleanup_status['uncleaned_count']} ä¸ªä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†")
            
        if not risk_items:
            report.append("  âœ“ ä½é£é™©: å†…å­˜å¢é•¿åœ¨å¯æ¥å—èŒƒå›´ï¼Œä¸´æ—¶æ–‡ä»¶ç®¡ç†è‰¯å¥½")
        else:
            for item in risk_items:
                report.append(f"  {item}")
            
        report.append("=" * 80)
        return "\n".join(report)


# ============================================================================
#                              æµ‹è¯•ç”¨ä¾‹
# ============================================================================

async def test_temp_file_creation():
    """æµ‹è¯•1: ä¸´æ—¶æ–‡ä»¶åˆ›å»º"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•1: ä¸´æ—¶æ–‡ä»¶åˆ›å»ºä¸æ¸…ç†")
    print("=" * 80)
    
    detector = MemoryLeakDetector()
    detector.start_tracking()
    detector.take_snapshot("åˆå§‹çŠ¶æ€")
    
    print("\nåˆ›å»º 20 ä¸ªä¸´æ—¶ Python æ–‡ä»¶...")
    for i in range(20):
        code = f"print('test {i}')\nimport time\ntime.sleep(0.01)"
        py_file = _ensure_py_file(code)
        detector.track_temp_file(py_file)
        
    detector.take_snapshot("åˆ›å»º20ä¸ªæ–‡ä»¶å")
    
    # æ£€æŸ¥æ–‡ä»¶æ¸…ç†
    cleanup_status = detector.check_temp_file_cleanup()
    print(f"\nä¸´æ—¶æ–‡ä»¶çŠ¶æ€:")
    print(f"  åˆ›å»º: {len(detector.temp_files_created)} ä¸ª")
    print(f"  æœªæ¸…ç†: {cleanup_status['uncleaned_count']} ä¸ª")
    print(f"  å ç”¨: {cleanup_status['total_size_mb']:.2f} MB")
    
    print("\n" + detector.generate_report())
    detector.stop_tracking()
    
    return cleanup_status


async def test_debug_sample_creation():
    """æµ‹è¯•2: è°ƒè¯•æ ·æœ¬åˆ›å»º"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: è°ƒè¯•æ ·æœ¬æ–‡ä»¶åˆ›å»º")
    print("=" * 80)
    
    detector = MemoryLeakDetector()
    detector.start_tracking()
    detector.take_snapshot("åˆå§‹çŠ¶æ€")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
    test_files = {}
    
    # CSV
    csv_file = Path(tempfile.gettempdir()) / "test_data.csv"
    csv_file.write_text("col1,col2,col3\n" + "\n".join(
        [f"{i},{i*2},{i*3}" for i in range(1000)]
    ))
    detector.track_temp_file(csv_file)
    test_files['csv'] = csv_file
    
    # JSONL
    jsonl_file = Path(tempfile.gettempdir()) / "test_data.jsonl"
    with jsonl_file.open('w') as f:
        for i in range(1000):
            f.write(json.dumps({'id': i, 'value': i*2}) + '\n')
    detector.track_temp_file(jsonl_file)
    test_files['jsonl'] = jsonl_file
    
    # JSON Array
    json_file = Path(tempfile.gettempdir()) / "test_data.json"
    json_file.write_text(json.dumps([{'id': i, 'value': i*2} for i in range(1000)]))
    detector.track_temp_file(json_file)
    test_files['json'] = json_file
    
    detector.take_snapshot("åˆ›å»ºæµ‹è¯•æ–‡ä»¶å")
    
    # åˆ›å»ºæ ·æœ¬
    print("\nåˆ›å»ºæ ·æœ¬æ–‡ä»¶...")
    for fmt, file_path in test_files.items():
        for i in range(3):
            sample_file = _create_debug_sample(file_path, sample_lines=10)
            detector.track_temp_file(sample_file)
            print(f"  {fmt} æ ·æœ¬ {i+1}: {sample_file.stat().st_size} bytes")
    
    detector.take_snapshot("åˆ›å»ºæ‰€æœ‰æ ·æœ¬å")
    
    # æ¯”è¾ƒå†…å­˜
    detector.compare_snapshots(0, -1, top_n=5)
    
    print("\n" + detector.generate_report())
    detector.stop_tracking()


async def test_subprocess_execution():
    """æµ‹è¯•3: å­è¿›ç¨‹æ‰§è¡Œ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: å­è¿›ç¨‹æ‰§è¡Œä¸èµ„æºæ¸…ç†")
    print("=" * 80)
    
    detector = MemoryLeakDetector()
    detector.start_tracking()
    detector.take_snapshot("å¼€å§‹")
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    script = _ensure_py_file("""
import sys
import time
print("Script started", file=sys.stderr)
for i in range(3):
    print(f"Iteration {i}")
    sys.stdout.flush()
    time.sleep(0.1)
print("Script finished", file=sys.stderr)
""")
    detector.track_temp_file(script)
    
    # æ£€æŸ¥åˆå§‹å­è¿›ç¨‹æ•°
    initial_children = len(psutil.Process().children())
    print(f"\nåˆå§‹å­è¿›ç¨‹æ•°: {initial_children}")
    
    # æ‰§è¡Œå¤šæ¬¡
    print("\næ‰§è¡Œè„šæœ¬ 15 æ¬¡...")
    for i in range(15):
        result = await _run_py(script)
        current_children = len(psutil.Process().children())
        
        if i % 5 == 0:
            print(f"  è¿­ä»£ {i}: å­è¿›ç¨‹={current_children}, "
                  f"è¿”å›ç ={result['return_code']}, "
                  f"stdoutè¡Œæ•°={len(result['stdout'].splitlines())}")
            detector.take_snapshot(f"æ‰§è¡Œ{i}æ¬¡å")
            
        # ä¸ä¿ç•™å¼•ç”¨
        del result
        
    gc.collect()
    await asyncio.sleep(0.5)  # ç­‰å¾…å­è¿›ç¨‹å®Œå…¨é€€å‡º
    
    final_children = len(psutil.Process().children())
    print(f"\næœ€ç»ˆå­è¿›ç¨‹æ•°: {final_children}")
    
    if final_children > initial_children:
        print(f"âš ï¸  è­¦å‘Š: å­è¿›ç¨‹æ³„æ¼! å¢åŠ äº† {final_children - initial_children} ä¸ª")
    else:
        print("âœ“ å­è¿›ç¨‹æ­£å¸¸æ¸…ç†")
        
    detector.take_snapshot("æ‰€æœ‰æ‰§è¡Œå®Œæˆå")
    detector.compare_snapshots(0, -1, top_n=5)
    
    print("\n" + detector.generate_report())
    detector.stop_tracking()


async def test_large_file_processing():
    """æµ‹è¯•4: å¤§æ–‡ä»¶å¤„ç†"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: å¤§æ–‡ä»¶å¤„ç†å†…å­˜ä½¿ç”¨")
    print("=" * 80)
    
    detector = MemoryLeakDetector()
    detector.start_tracking()
    detector.take_snapshot("å¼€å§‹")
    
    # åˆ›å»ºå¤§ CSV
    print("\nåˆ›å»ºå¤§æ–‡ä»¶ (10ä¸‡è¡Œ)...")
    large_csv = Path(tempfile.gettempdir()) / "large_test.csv"
    with large_csv.open('w') as f:
        f.write("id,value1,value2,value3,text\n")
        for i in range(100000):
            f.write(f"{i},{i*2},{i*3},{i*4},text_{i}\n")
    
    file_size_mb = large_csv.stat().st_size / 1024 / 1024
    print(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
    detector.track_temp_file(large_csv)
    detector.take_snapshot("åˆ›å»ºå¤§æ–‡ä»¶å")
    
    # æµ‹è¯•é‡‡æ · - å¤šæ¬¡
    print("\nåˆ›å»º 5 ä¸ªæ ·æœ¬...")
    for i in range(5):
        sample = _create_debug_sample(large_csv, sample_lines=10)
        detector.track_temp_file(sample)
        print(f"  æ ·æœ¬ {i+1}: {sample.stat().st_size / 1024:.2f} KB")
        
        if i % 2 == 0:
            detector.take_snapshot(f"åˆ›å»ºæ ·æœ¬{i+1}å")
        
        del sample
    
    gc.collect()
    detector.take_snapshot("æ¸…ç†å¼•ç”¨å")
    
    # æ¯”è¾ƒ
    detector.compare_snapshots(1, -1, top_n=5)
    
    print("\n" + detector.generate_report())
    detector.stop_tracking()
    
    # æ¸…ç†
    if large_csv.exists():
        large_csv.unlink()
        print(f"\nå·²æ¸…ç†å¤§æ–‡ä»¶")


async def test_state_like_accumulation():
    """æµ‹è¯•5: æ¨¡æ‹Ÿ state æ•°æ®ç´¯ç§¯"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: State æ•°æ®ç´¯ç§¯æ¨¡æ‹Ÿ")
    print("=" * 80)
    
    detector = MemoryLeakDetector()
    detector.start_tracking()
    
    # æ¨¡æ‹Ÿ state
    state_data = {}
    detector.take_snapshot("ç©ºstate")
    
    print("\næ¨¡æ‹Ÿ 100 æ¬¡æ“ä½œ...")
    for i in range(100):
        # æ¨¡æ‹Ÿæ·»åŠ æ•°æ®
        state_data[f'key_{i}'] = 'x' * 10000  # 10KB
        state_data[f'code_{i}'] = 'def func():\n    pass\n' * 500  # ~15KB
        
        if i % 20 == 0:
            size_kb = sum(sys.getsizeof(v) for v in state_data.values()) / 1024
            print(f"  è¿­ä»£ {i}: keys={len(state_data)}, æ€»å¤§å°={size_kb:.2f} KB")
            detector.take_snapshot(f"è¿­ä»£{i}")
    
    detector.take_snapshot("100æ¬¡è¿­ä»£å")
    
    # æ¸…ç†
    print("\næ¸…ç† state_data...")
    state_data.clear()
    gc.collect()
    
    detector.take_snapshot("æ¸…ç†å")
    
    detector.compare_snapshots(0, -2, top_n=5)  # æ¸…ç†å‰
    detector.compare_snapshots(-2, -1, top_n=5)  # æ¸…ç†å
    
    print("\n" + detector.generate_report())
    detector.stop_tracking()


# ============================================================================
#                              ä¸»å…¥å£
# ============================================================================

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ” " * 20)
    print("å¼€å§‹å†…å­˜æ³„æ¼æ£€æµ‹")
    print("ğŸ” " * 20)
    
    tests = [
        ("ä¸´æ—¶æ–‡ä»¶åˆ›å»º", test_temp_file_creation),
        ("è°ƒè¯•æ ·æœ¬åˆ›å»º", test_debug_sample_creation),
        ("å­è¿›ç¨‹æ‰§è¡Œ", test_subprocess_execution),
        ("å¤§æ–‡ä»¶å¤„ç†", test_large_file_processing),
        ("Stateç´¯ç§¯", test_state_like_accumulation),
    ]
    
    results = {}
    
    for name, test_func in tests:
        try:
            result = await test_func()
            results[name] = {"status": "âœ“ é€šè¿‡", "result": result}
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results[name] = {"status": "âœ— å¤±è´¥", "error": str(e)}
        
        # æ¯ä¸ªæµ‹è¯•ä¹‹é—´æ¸…ç†
        gc.collect()
        await asyncio.sleep(1)
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 80)
    for name, result in results.items():
        print(f"{result['status']} - {name}")
    
    print("\n" + "ğŸ” " * 20)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ” " * 20)


if __name__ == "__main__":
    asyncio.run(run_all_tests())