2025-12-10 16:48:39 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:126 | The title of the paper is Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework

Here's first ten page content: arXiv:2506.02454v1  [cs.CL]  3 Jun 2025
Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
From Scratch with Agentic Framework
Zhaorui Yang§*, Bo Pan§*, Han Wang§*, Yiyao Wang§, Xingyu Liu§
Minfeng Zhu‡B, Bo Zhang§B, Wei Chen§
§State Key Lab of CAD&CG, Zhejiang University
‡Zhejiang University
https://rickyang1114.github.io/multimodal-deepresearcher/
Abstract
Visualizations play a crucial part in effective
communication of concepts and information.
Recent advances in reasoning and retrieval aug-
mented generation have enabled Large Lan-
guage Models (LLMs) to perform deep re-
search and generate comprehensive reports. De-
spite its progress, existing deep research frame-
works primarily focus on generating text-only
content, leaving the automated generation of
interleaved texts and visualizations underex-
plored. This novel task poses key challenges
in designing informative visualizations and ef-
fectively integrating them with text reports. To
address these challenges, we propose Formal
Description of Visualization (FDV), a struc-
tured textual representation of charts that en-
ables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this
representation, we introduce Multimodal Deep-
Researcher, an agentic framework that decom-
poses the task into four stages: (1) researching,
(2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the
evaluation of generated multimodal reports, we
develop MultimodalReportBench, which con-
tains 100 diverse topics served as inputs along
with 5 dedicated metrics. Extensive experi-
ments across models and evaluation methods
demonstrate the effectiveness of Multimodal
DeepResearcher. Notably, utilizing the same
Claude 3.7 Sonnet model, Multimodal DeepRe-
searcher achieves an 82% overall win rate over
the baseline method.
1
Introduction
Large language models (LLMs) have demonstrated
broad capabilities in solving diverse tasks such as
question answering, coding and math (Bai et al.,
2022; Guo et al., 2025; Huang et al., 2024). Aug-
mented with searching and reasoning capabili-
ties (Xie et al., 2023; Nakano et al., 2021; Li et al.,
*Equal Contribution
BCorresponding Authors
Under review. Code will be released upon acceptance.
This remarkable pace of activity resulted in a record 2,664 cataloged objects 
entering Earth's orbit—surpassing any previous year in spaceflight history. [...]
Figure 1: A text-chart interleaved snippet from the re-
port generated by our Multimodal DeepResearcher.
2025a), LLMs can perform deep research and effec-
tively leverage up-to-date external information be-
yond static parameters (Li et al., 2025a). Recently,
this paradigm has garnered significant attention
with its remarkable efficacy in generating grounded,
comprehensive reports from scratch (Shao et al.,
2024; Huot et al., 2025). However, existing deep
research frameworks from both academia (Jin et al.,
2025; Zheng et al., 2025b; Li et al., 2025b) and in-
dustry (OpenAI, 2025c; Google, 2024; xAI, 2025;
David Zhang, 2025) predominantly focus on gen-
erating text-only content, neglecting the display
beyond text modality. The text-heavy nature of
these reports impedes effective communication of
concepts and information (Ku et al., 2025; Zheng
et al., 2025a), which limits their readability and
practical utility.
In real-world scenarios, visualization serves as
a crucial part of reports and presentations, offer-
ing remarkable capabilities for conveying data in-
sights (Otten et al., 2015), facilitating the identifica-
tion of implicit patterns (Yang et al., 2024), and en-
hancing audience engagement (Barrick et al., 2018;
Zheng et al., 2025a). Human experts typically craft
1
meticulously designed visualizations with consis-
tent styles to effectively communicate ideas and
insights. They then integrate these visualizations
within appropriate textual context (He et al., 2024)
to create coherent text-chart interleaved reports.
However, the end-to-end generation of multi-
modal reports remains challenging.
Although
prompting LLMs to generate individual visualiza-
tion charts is a promising solution (Yang et al.,
2024; Seo et al., 2025; Han et al., 2023), effec-
tively representing and integrating these visualiza-
tions with textual content poses significant chal-
lenges. Although in-context learning appears to be
a promising approach for guiding such generation,
the absence of a standardized format for text-chart
interleaved content impedes effective implementa-
tion of in-context learning strategies.
To address this challenge, we introduce the For-
mal Description of Visualization (FDV), a struc-
tured representation method inspired by the gram-
mar of graphics theory (Wilkinson, 1999). FDV
comprehensively captures visualization designs
through four perspectives (i.e., overall layout, plot-
ting scale, data, and marks). This representation
provides universal and high-fidelity descriptions
that enables in-context learning of human expert
designs and produce charts of professional quality.
Building upon FDV, we introduce Multimodal
DeepResearcher, an agentic framework that gener-
ates text-chart interleaved reports from scratch. A
snippet of generated report is illustrated in Figure 1.
The framework operates through four stages: (1)
researching, which gathers comprehensive infor-
mation through searching and reasoning; (2) exem-
plar report textualization, which textualizes multi-
modal reports from human experts using our pro-
posed Formal Description of Visualization (FDV,
Section 3.2) for in-context learning; (3) planning,
which establishes a content outline and visualiza-
tion style guide to ensure consistency throughout
the report; and (4) multimodal report generation,
which produces the final interleaved report through
drafting, coding and iterative chart refinement.
We evaluate our framework with Multimodal-
Bench (Section 4.1), which comprises 100 topics
used as inputs and 5 dedicated evaluation metrics.
Our experiments include both proprietary and open-
source models with automatic and human evalua-
tion. As a baseline, we adapted DataNarrative (Is-
lam et al., 2024), a relevant framework that gener-
ates placeholders for charts from tabular inputs, to
perform our task. Both automatic and human evalu-
ations consistently demonstrate Multimodal Deep-
Researcher’s superior performance compared to
the baseline. Notably, when using Claude 3.7 Son-
net as the generator, Multimodal DeepResearcher
achieves an impressive 82% overall win rate.
Our contributions can be summarized as follows:
• We propose a novel task that generates a
text-chart interleaved multimodal report from
scratch and a corresponding dataset and eval-
uation metrics.
• We propose Formal Description of Visualiza-
tion, a structured textual representation of vi-
sualizations that enables the in-context learn-
ing and generation of multimodal reports.
• We introduce Multimodal DeepResearcher,
an end-to-end agentic framework that gener-
ates high-quality multimodal reports, which
largely outperform the baseline method.
2
Related Work
Deep Research
Recently, the combination of re-
trieval techniques (Li et al., 2025c; Zhao et al.,
2024) and reasoning (Guo et al., 2025) has enabled
LLMs to transcend their parametric constraints by
leveraging external knowledge. Pioneering works
have designed specialized prompts and workflows
for complex research tasks, as exemplified by Open-
Researcher (Zheng et al., 2024) and Search-o1 (Li
et al., 2025a). Subsequent research has explored
reinforcement learning for end-to-end reasoning
and information retrieval (Jin et al., 2025; Zheng
et al., 2025b). However, these approaches primar-
ily focus on generating and evaluating text-only
results, whereas our study advances the field by
generating text-chart interleaved reports that sig-
nificantly enhance information comprehension and
communication with visualizations.
LLM for Data Visualizations
Current work
has focused on enhancing individual chart quality
through various approaches, including multi-stage
pipelines (Dibia, 2023), iterative debugging with vi-
sual feedback (Yang et al., 2024), chain-of-thought
prompted query reformulation (Seo et al., 2025),
and models fine-tuned with domain-specific data
for chart generation (Han et al., 2023; Tian et al.,
2024). Other research has explored how to articu-
late generation intent, such as multimodal prompt-
ing with sketches and direct manipulations (Wen
2
“Tell me 
about xxx.”
Knowledge
Exemplar
Reports
FDV
Outline
Visualization
Style Guide
Researching A
Exemplar Textualization B
Report Generation D
Final Report
Layout Scale
Data
Marks
Planning C
Textual Report
Text
Information
&
References
Drafting
Coding
In-context
Learning
MLLM
Refining
Figure 2: The framework of the Multimodal DeepResearcher. It decomposes the task of multimodal report
generation into four stages: A Iterative researching about given topic; B Exemplar textualization of human experts
using proposed Formal Description of Visualization (FDV, Section 3.2); C Planning; D Report Generation, which
generates the final report with crafting, coding and iterative refinement.
et al., 2025), multilingual natural language inter-
faces (Maddigan and Susnjak, 2023), and conver-
sational context management (Hong and Crisan,
2023). Corresponding evaluation methodologies
have also been proposed (Li et al., 2024a; Chen
et al., 2025). Unlike previous work that focuses
predominantly on single chart or limited data and
chart types, our work is the first to generate and
evaluate text-chart interleaved reports, which con-
tains multiple diverse visualizations based on in-
the-wild complex and heterogeneous information.
LLM for agentic generation
LLMs have been
widely applied to various generation tasks due to
their ability to process complex textual informa-
tion (Ku et al., 2024; Nijkamp et al., 2023b,a;
Jimenez et al., 2024; Yang et al., 2025b). For more
challenging tasks, researchers have designed LLM
agents that decompose problems into reasoning,
planning, and execution stages (Luo et al., 2025).
These agents have demonstrated remarkable suc-
cess across scientific research (Lu et al., 2024; Si
et al., 2024; Li et al., 2024b; Bogin et al., 2024),
video generation (He et al., 2025), and computer
system interaction (Xie et al., 2024; Deng et al.,
2023; Zhang et al., 2023). This paradigm extends
effectively to the visualization domain as well. The-
oremExplainAgent (Ku et al., 2025) uses agents to
generate educational videos, and PPTAgent (Zheng
et al., 2025a) automatically creates presentations in
the form of slides with integrated text and visuals.
Most relevant to our work, DataNarrative (Islam
et al., 2024) explores generating simple specifica-
tions for data-driven visualizations and evaluating
these specifications as proxies for visualization as-
sessment. However, this approach remains limited
to simple chart types (e.g., bar chart or line chart),
which restricts their practical use.
3
Method
We formulate the task of multimodal report genera-
tion as follows: given a topic t and a set of multi-
modal exemplar reports R containing interleaved
textual content and charts, the system is expected to
output a multimodal report as in R based on t. To
solve this task, we introduce Multimodal DeepRe-
searcher, an agentic framework which decomposes
the task into four steps: (1) researching through it-
erative web search and reasoning (Section 3.1); (2)
exemplar report textualization (Section 3.2), which
textualizes multimodal exemplar reports from hu-
man experts using our proposed Formal Descrip-
tion of Visualization (FDV, Section 3.2); (3) plan-
ning (Section 3.3); and (4) Multimodal report gen-
eration (Section 3.4). An overview of Multimodal
DeepResearcher is presented in Figure 2.
3.1
Researching
To leverage up-to-date information beyond para-
metric knowledge, Multimodal DeepResearcher
conducts iterative research on a given topic t, gen-
erating a comprehensive set of learnings L. These
learnings encompass both information acquired
through web sources and their corresponding ref-
erences. The process involves iterative execution
of two primary operations: (1) web search and
(2) subsequent reasoning based on search results.
Initially, the agent prompts the LLM to generate
relevant keywords K = k1, · · · , knK based on
the given topic t. The agent then conducts web
3
- The visualization consists of two similar strip plots stacked vertically. 
- Each plot has a title at the top ('UK City Traffic Volume' and 'US City Traffic Volume'). 
- The overall chart has a shared legend at the bottom showing 'UK City Mean' and 'US City 
Mean' with corresponding colored lines. 
- Each plot has adequate margins on all sides, with city names aligned on the left side.
- X-axis: Linear scale from 0 to 9, representing 'Vehicles per hour (thousands)'. 
- X-axis has grid lines at 1-unit intervals (1, 2, 3, etc.). 
- X-axis label 'Vehicles per hour (thousands)' is placed at the bottom of each plot. 
- Y-axis: Categorical scale showing city names, evenly spaced vertically. 
- No y-axis title is shown, just the city names as tick labels aligned to the left.
- Color: All marks shown in grey by default, except that values above the UK mean for UK 
cities are marked in red, and values above the US mean for US cities are marked in blue.
- For each city, there are multiple traffic volume measurements, represented as small marks. 
- The mean traffic volume for each country (UK and US) is calculated and visualized as 
vertical lines.
- Small tick marks (resembling small vertical lines) represent individual traffic volume 
measurements for each city, with colors indicating both the country and whether values are 
above or below the mean. 
- A vertical red line represents the UK mean traffic volume in the top plot. 
- A vertical blue line represents the US mean traffic volume in the bottom plot.
Extract
Design
 
Implement
Design
(B) Formal Description of Visualization 
(A) Origin Visualization
(C) Reconstructed Visualization
Figure 3: The illustration Formal Description of Visualization (FDV) for the exemplar textualization process.
(A) Original traffic volume visualizations for UK and US cities; (B) The Formal Description of Visualization
(FDV) that systematically captures the visualization’s layout, scale, data, and marks using a structured format;
and (C) The reconstructed visualization based on the formal description. This process textualizes high-quality
text-chart interleaved reports by transforming visual elements into structured textual representations that preserve
the visualization’s essential characteristics.
searches using these keywords and retrieves web-
pages P = p1, · · · , pnP . Subsequently, the agent
analyzes these webpages, synthesizes the informa-
tion into learnings L, and formulates a research
question q for the next iteration. Based on this re-
search question and the original topic, the research
agent performs the next research cycle. After nR
rounds of iteration, the researcher produces a final
compilation of learnings. Further details of this
process are provided in Appendix A.1.
3.2
Exemplar Textualization
Human experts typically produce reports with both
texts and visualizations to enhance communication
and audience engagement (Zheng et al., 2025a;
Yang et al., 2024). To generate high-quality mul-
timodal content comparable to expert-created re-
ports, we employ in-context learning with exemplar
reports crafted by human experts. This approach
necessitates an effective methodology for convert-
ing multimodal exemplar reports R into textual
exemplar reports ˜R.
To address this challenge, we propose Formal
Description of Visualization (FDV), a structured
description method for visualization charts inspired
by the grammar of graphics (GoG) theory (Wilkin-
son, 1999), which theoretically provides universal
and high-fidelity descriptions for any visualization
designs. As shown in Figure 3 (B), FDV charac-
terizes each visualization chart from four perspec-
tives: (1) Overall layout, detailing the constituent
subplots and their spatial arrangements; (2) Plot-
ting scale, describing the scaling logic behind each
“data to visual channel (e.g., position, color)” map-
ping and their annotations; (3) Data, describing
both the numeric data and text elements used to gen-
erate the visualization. (4) Marks, describing the
design specifications of each visual element. The
reverse process of textualization can be achieved
via coding, which reconstructs the visualization
from FDV, as shown in Figure 3 (C).
In the practical implementation of textualization,
Multimodal DeepResearcher first extracts all vi-
sualization charts from the report, then prompts a
multimodal large language model to textualize and
replace each of them. This process is presented in
Algorithm 1. The full prompt for the textualization
process is provided in appendix B.2.
3.3
Planning
After iterative researching about the topic t through
nR rounds, Multimodal DeepResearcher creates a
plan before the actual generation process. Specif-
ically, it constructs an outline O that establishes
the narrative architecture of the report based on the
learnings L, topic t and textual exemplar report ˜R.
The outline comprises a hierarchical structure of
sections, each containing a descriptive title and a
brief summary. To learn the style of visualizations
present in exemplar reports ˜R and maintain a con-
sistent style in the generation of each visualization
chart, Multimodal DeepResearcher also prompts
the LLM to generate a visualization style guide G.
The visualization style guide provides guidelines
4
Algorithm 1 Textualization of multimodal reports
1: Inputs: Multimodal exemplar reports R.
2: Requires: Multimodal large language model
Mv, replace function replace.
3: Outputs: Textualized exemplar reports ˜R.
4: Initialize ˜R = ∅
5: for r in R do
6:
Init. ˜r = r
7:
for each image i in r do
8:
// Extract FDV from image
9:
FDVi = Mv(i)
10:
// Replace image with extracted FDV
11:
˜r = ˜r.replace(i, FDVi)
12:
end for
13:
˜R = ˜R ∪{˜r}
14: end for
15: Return: ˜R
that control the overall style of visualizations in the
report (e.g., color palette, font hierarchy). More
details of this process can be found in appendix A.
3.4
Generating the Final Report
The final stage of Multimodal DeepResearcher is
responsible for the actual generation of the multi-
modal report with interleaved textual content and
visualizations. The report is generated with out-
puts of previous stages, i.e., learnings L, exemplar
textual reports ˜R, outline O and visualization style
guide G.
Multimodal DeepResearcher first prompts the
LLM to generate a textual report with Formal De-
scription of Visualization (FDV) defined in Sec-
tion 3.2 as a placeholder for the underlying visu-
alization chart to be generated. The format of this
textual report is expected to be the same as those in
textual exemplar reports used for in-context learn-
ing. Then, Multimodal DeepResearcher extracts
all occurrences of FDVs, and prompts the LLM to
implement the design via coding. Since visualiza-
tions represented by FDV have extensive flexibility,
which may exceed the expressive capabilities of
typical declarative visualization libraries (Heer and
Bostock, 2010) (e.g., matplotlib), we directed the
LLMs to utilize D3 , the most widely used im-
perative visualization programming library based
on JavaScript and HTML, to implement the target
visualization designs.
To further improve the quality of visualizations
generated, Multimodal DeepResearcher includes
https://d3js.org
an actor-critic mechanism to revise the code for
generating the visualization charts motivated by
recent advancements of agents (Yang et al., 2024).
In this scenario, the actor is the LLM Mt responsi-
ble for generating the code, and the critic feedback
comes from both the console and a critic model.
Console feedback is collected using chrome de-
veloper tools provided as Python package to sim-
ulate a browser. It first collects console messages
that may contain errors during the loading of visu-
alizations. After all elements are loaded, it takes a
screenshot to obtain the visualization chart.
After getting the screenshot of the visualization
chart, Multimodal DeepResearcher employs a mul-
timodal LLM Mv to serve as a critic, which pro-
vides visual feedback. The multimodal LLM takes
the chart rendered as input, examines the visual
quality, and delivers corresponding feedback. It
further determines whether the current chart needs
improvement. If improvement is needed, the actor
refines its code based on the feedback and console
message. This iterative refinement continues until
the critic is satisfied, or the predefined upper limit
of retry times is reached, which we set as 3 to avoid
infinite refinement cycles. When the refinement
process finishes, the critic selects the final chart
from the final two iterations.
The algorithm for the refine process is presented
in Algorithm 2. The prompts employed during
this process are detailed in appendix B.5. A com-
prehensive full report generated by Multimodal
DeepResearcher is presented in Appendix E.
4
Experiments
In this section, we present the MultimodalReport-
Bench and corresponding evaluation criteria for
evaluation, followed by the experimental results.
4.1
Data Selection
To systematically evaluate the multimodal report
generated by Multimodal DeepResearcher, we con-
structed a dataset comprising 100 real-world topics
curated from public websites that feature multi-
modal reports crafted by human experts, i.e., Pew
Research (Pew, 2025), Our World in Data (OWID,
2025) and Open Knowledge Foundation (OKF,
2024). Pew Research informs the public about is-
sues, attitudes and trends shaping the world through
research report. Our World in Data presents em-
pirical data and research on global development
https://developer.chrome.com/docs/devtools
5
Algorithm 2 Algorithm for refining charts
1: Inputs: chart c represented as code.
2: Requires: Browser tool T, LLM Mt, Multi-
modal LLM Mv.
3: Outputs: Refined chart ˜c.
4: Hypars: Number of max retry times Nmax.
5: Initialize satisfied = False, c0 = c, C = {c}.
6: for i = 1 to Nmax do
7:
// Get console message and image
8:
msg, i = T(c)
9:
// Critic Mv evaluates the chart
10:
satisfied, feedback = Mv(i)
11:
if satisfied == True then
12:
break
13:
end if
14:
// actor Mt refines previous chart
15:
ci = Mt(ci−1, msg, feedback)
16:
C = C ∪{ci}
17: end for
18: ˜c = c0
19: if |C| > 1 then
20:
// Selects from the last two charts
21:
˜c = Mv(C[−1], C[−2])
22: end if
23: Return: ˜c
challenges through web publications. The Open
Knowledge Foundation is dedicated to promoting
open data and content across all domains, ensuring
information accessibility. These sources contain
exemplary multimodal reports, making their topics
appropriate for our evaluation task.
The topics are then used as inputs for multimodal
report generation. To ensure that our dataset applies
to the real-world scenario, we meticulously curated
topics spanning 10 categories, such as travel, en-
ergy and education. Table 4 presents the distri-
bution of topics. We also collected 6 multimodal
reports with no overlapping in topics to serve as ex-
emplar reports for in-context learning, as described
in Section 3.2.
4.2
Baseline Selection
Our task requires generating a multimodal re-
port from scratch, which is infeasible with direct
prompting or existing deep research frameworks.
Most similar to our work, DataNarrative (Islam
et al., 2024) generates simple data-driven visual-
ization specifications based on data tables as input,
and evaluates the textual specification as a proxy of
chart. We incorporate our researching module and
adapt its framework accordingly to establish our
baseline. For an apple-to-apple comparison, we
utilize the learnings generated with our researching
stage (Section 3.1) and plans (Section 3.3) instead
of tables as the input. It then goes through generate-
verify-refine process, consistent with the original
framework. Since the original framework lacks
mechanisms for transforming design specifications
into actual charts, we extract all design specifi-
cations and generate corresponding visualizations
using the same pipeline as Multimodal Researcher
does in Section 3.4.
4.3
Framework Implementation
Multimodal DeepResearcher is an agentic frame-
work with multiple stages. In this section, we de-
scribe the implementation details of each stage.
In the researching stage (Section 3.1), we perform
web search and scrape with Firecrawl API, and con-
duct reasoning with GPT-4o-mini (OpenAI, 2025a).
GPT-4o-mini is also utilized for planning. Claude
3.7 Sonnet (Anthropic, 2025) is utilized as the
MLLM for the textualization of exemplar reports
(Section 3.2). The generation of the final multi-
modal report requires both a large language model
to craft textual report, and a multimodal large lan-
guage model to provide visual feedback for the
chart. Our experiments encompasses two configura-
tions: (1) State-of-the-art proprietary models, with
Claude 3.7 Sonnet serving as both the LLM and
multimodal LLM. (2) Open-source models, specifi-
cally Qwen3-235B-A22B (Yang et al., 2025a) and
Qwen2.5-VL-72B-Instruct (Bai et al., 2025). To
ensure fair comparison, all the settings are consis-
tent in both Multimodal DeepResearcher and the
DataNarrative baseline where applicable. All calls
were made from OpenRouter with no GPU utilized
in our experiments.
4.4
Automatic Evaluation
Given the multimodal nature of the outputs in our
task, evaluation necessitates assessment of both
texts and visualizations. To accomplish this, we
convert the visualizations generated into base64
encoding, and prompt a Multimodal LLM to con-
duct head-to-head comparisons of two reports with
the format of OpenAI messages. Specifically, we
utilized GPT-4.1 (OpenAI, 2025b) as the evalua-
https://www.firecrawl.dev/
https://openrouter.ai/
https://platform.openai.com/docs/
api-reference/images
6
tor in all automatic evaluation experiments. Since
report generation constitutes an open-ended, objec-
tive task, reference-based metrics typically fail to
align with human-perceived standards (Liu et al.,
2023). Therefore, we established a comprehensive
criteria incorporating both texts and visualizations
in reports, which primarily consists of five metrics:
Informativeness and Depth. Evaluates whether
the report delivers comprehensive, substantive and
thorough information through both texts and ac-
company visualizations.
Coherence and Organization.
Evaluates
whether the report is well-organized, and whether
the visualizations connect meaningfully to the text.
Verifiability. Evaluates whether the information
of the reports can be verified with citations. Apart
from textual links to references, we also prompt
the evaluator to check the annotation present in
visualizations that may contain source information.
Visualization Quality. Evaluates the quality of
visualization charts in the report, including visual
clarity and textual labels and annotations.
Visualization Consistency. Evaluates whether
the visualizations in the report maintain a consistent
overall style. The style contains the color palettes,
typography and information hierarchy in visualiza-
tions.
During evaluation, we provide the evaluator with
the topic, learnings which contain both knowledge
acquired through web search and corresponding ref-
erences and both reports. Specifically, we prompt
the evaluator to rate both reports between on a 1-5
scale with detailed guides, subsequently comparing
scores to determine superiority or equivalence. To
mitigate positional bias, we randomize the presen-
tation order of reports. The complete evaluation
prompt is provided at appendix B.6.
Results. As illustrated in Table 1, Multimodal
DeepResearcher consistently outperforms Data-
Narrativeacross across both proprietary and open-
source model configurations. With Claude 3.7 Son-
net, it achieves an overall win rate of 82%. Specif-
ically, Multimodal DeepResearcher outperforms
with a high win rate in Verifiability (86%), Vi-
sualization Quality (80%) and Visualization con-
sistency (78%).
A similar pattern is observed
with open-source models (Qwen3-235B-A22B and
Qwen2.5-VL-72B-Instruct), where Multimodal
DeepResearcher achieves a win rate of 55%. The
results demonstrate the efficacy of Multimodal
DeepResearcher in generating multimodal reports.
Table 1: Automatic evaluation results of the multimodal
report: Multimodal DeepResearcher (Ours) vs. Data-
Narrative.
Ours vs DataNarrative
Evaluation Metrics
Ours Win
Ours Lose
Tie
w. Claude 3.7 Sonnet
Informativeness and Depth
75%
25%
0%
Coherence and Organization
76%
21%
3%
Verifiability
86%
5%
9%
Visualization Quality
80%
16%
4%
Visualization Consistency
78%
17%
5%
Overall
82%
16%
2%
w. Qwen3-235B-A22B & Qwen2.5-VL-72B-Instruct
Informativeness and Depth
50%
50%
0%
Coherence and Organization
41%
51%
8%
Verifiability
66%
21%
13%
Visualization Quality
48%
46%
6%
Visualization Consistency
52%
42%
6%
Overall
55%
40%
5%
Table 2: Human evaluation of the generated reports:
Multimodal DeepResearcher (Ours) vs. DataNarrative.
Evaluation Metrics
Ours Win
Ours Lose
Tie
Informativeness and Depth
100%
0%
0%
Coherence and Organization
100%
0%
0%
Verifiability
100%
0%
0%
Visualization Quality
80%
10%
10%
Visualization Consistency
80%
10%
10%
Overall
100%
0%
0%
4.5
Human Evaluation
For human evaluation, we utilized the same set of
metrics as in automatic evaluation. We selected a
random subset of 10 topics for evaluation. Specifi-
cally, 3 annotators performed pairwise comparison
of reports generated by both Multimodal DeepRe-
searcher and DataNarrative with Claude 3.7 Son-
net. As with automatic evaluation (Section 4.5),
we randomized report presentation order to avoid
positional bias. Results are presented in Table 2.
Surprisingly, Multimodal DeepResearcher achieves
an overall win rate of 100%. Specifically, two an-
notators preferred all 10 reports generated by Mul-
timodal DeepResearcher, while the third annotator
preferred 9 out of 10. The results further validate
the effectiveness of Multimodal Deepresearcher.
4.6
Ablation Studies
To assess the efficacy of individual components of
Multimodal DeepResearcher, we conducted abla-
tion experiments on a random subset of 20 topics.
Specifically, we compared 3 variants against Multi-
7
Table 3: Results of ablation studies across three differ-
ent setups. We report the lose, win and tie rates for
each setup against the complete framework. Claude 3.7
Sonnet serves as both the LLM and MLLM here.
Ablated Components
Lose
Win
Tie
- w/o Exemplar Learning
70%
20%
10%
- w/o Planning
85%
15%
0%
- w/o Refinement of charts
80%
20%
0%
modal DeepResearcher: (1) w/o in-context learning
from exemplar reports (Section 3.2); (2) w/o plan-
ning (Section 3.3); (3) w/o iterative refinement of
charts (Section 3.4). To ensure fair comparison,
all other settings and hyperparameters remained
consistent across variants. As shown in table 3,
removing any component results in significant per-
formance degradation. Specifically, eliminating ex-
emplar learning from human reports yields a 70%
lose rate, direct generation without planning leads
to 85% lose rate, and removing chart refinement
process loses in 80% cases. These findings demon-
strate the contribution of each component to the
effectiveness of Multimodal DeepResearcher.
5
Analysis
5.1
Visualization Analysis
In this section, we analyze the characteristics of
visualizations generated with Multimodal Deep-
Researcher and the baseline. While the average
number of charts per report between our frame-
work (9.3) and DataNarrative (9.4) is comparable,
the visualizations generated by Multimodal Deep-
Researcher are notably more diverse. As illustrated
in Figure 4, although both methods prioritized con-
ventional chart types such as bar charts and line
charts, Multimodal DeepResearcher demonstrates
superior capability in generating sophisticated and
complex visualizations.
For instance, across the 100 selected topics, Mul-
timodal DeepResearcher produces 15 flowcharts
and 18 dashboards, while DataNarrative generates
merely 2 flowcharts and 1 dashboard. Another
example involves the “Others” category, which en-
compasses hard-to-categorize visualizations such
as infographics and mind maps. Our framework
generates 280 such charts, substantially exceeding
the 96 produced by DataNarrative. This disparity
underscores our approach’s flexibility in accommo-
dating to diverse real-world scenarios. We provide
a collection of examples for each type generated
403 (43.9%)
262 (28.5%)
53 (5.8%)
54 (5.9%)
96 (10.4%)
DataNarrative
282 (31.1%)
209 (23.0%)
280 (30.9%)
Multimodal DeepResearcher
Bar chart
Line chart
Pie chart
Scatter chart
Bubble chart
Flowchart
Dashboard
Choropleth map
Sankey diagram
Others
Figure 4: Distribution of visualization charts generated
with DataNarrative and Multimodal DeepResearcher
(Ours). The first column in the legend (denoted by red
and yellow colors) represents conventional chart types.
by Multimodal DeepResearcher in appendix C.
5.2
Error Analysis
Despite the remarkable efficacy of Multimodal
DeepResearcher, the integration of visualizations
poses new challenges. In this section, we catego-
rize the identified common errors into the following
two categories.
Overlapping
Overlapping of elements is the
most common error. It is generally attributed to two
factors: (1) excessive information in FDV that com-
plicates proper arrangement within limited space.
(2) suboptimal placement of legends, labels and an-
notations. Illustrative examples of both scenarios
are provided in Appendix D.
Hallucination
Hallucination persists as a funda-
mental challenge for LLMs (Shao et al., 2024; Is-
lam et al., 2024), which also extends to the gener-
ation of visualizations. Figure 17 exemplifies this
issue through a choropleth map example, where the
model erroneously marked regions with inadequate
data with hallucinated content using red color.
6
Conclusion
In this work, we investigate the challenge of gen-
erating multimodal reports from scratch. We in-
troduce the Formal Description of Visualization,
a structured representation of charts that enables
in-context learning from human-created exemplar
reports. Based on this, we propose Multimodal
DeepResearcher, an end-to-end framework for the
generation of multimodal reports. While exten-
sive experiments with both automatic evaluation
and human evaluation confirm the efficacy of our
8
framework, challenges remain in improving visual-
ization quality and reducing hallucination.
Limitations
Although Multimodal DeepResearcher has demon-
strated remarkable potential in end-to-end genera-
tion of multimodal reports from scratch, the frame-
work contains limitations due to the complex nature
of the task. First, several types of errors exist in
the generated visualizations, as discussed in Sec-
tion 5.2. Furthermore, in-context learning from
exemplar reports imposes demands on context size
and understanding capabilities of LLMs. Moreover,
the considerable computational expenditure associ-
ated with state-of-the-art models, coupled with the
extensive processing time required for visualization
code generation, necessitated the utilization of a
relatively constrained dataset for experimental vali-
dation. The totality of these experiments incurred
approximately 650 USD in API usage fees.
Ethical Considerations
For human evaluation, we recruited three partici-
pants with extensive academic paper writing expe-
rience from a local university. Prior to the experi-
ment, participants were informed about the study
duration and general procedures, and consent to
participate. After human evaluation, all partici-
pants were compensated with rates above average
wage.
We also recognize and uphold the importance of
intellectual property. The data used by our Multi-
modal DeepResearcher system was obtained solely
from publicly accessible and legally permissible
sources, where academic utilization is approved.
All the selected topics are carefully checked to ex-
clude potentially offensive content.
We used large language models as auxiliary tools
to facilitate the writing of the manuscript, with
careful verification to ensure precision.
References
Anthropic. 2025. Claude. https://www.anthropic.
com/claude/sonnet.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-
bin Ge, Sibo Song, Kai Dang, Peng Wang, Shi-
jie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu,
Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei
Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others.
2025. Qwen2.5-vl technical report. arXiv preprint
arXiv:2502.13923.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, and 1
others. 2022. Training a helpful and harmless assis-
tant with reinforcement learning from human feed-
back. arXiv preprint arXiv:2204.05862.
Andrea Barrick, Dana Davis, and Dana Winkler. 2018.
Image versus text in powerpoint lectures: Who does
it benefit? Journal of Baccalaureate Social Work,
23(1):91–109.
Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle
Richardson, Erin Bransom, Peter Clark, Ashish Sab-
harwal, and Tushar Khot. 2024. Super: Evaluating
agents on setting up and executing tasks from re-
search repositories. Preprint, arXiv:2409.07440.
Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, and
Yuqing Yang. 2025. Viseval: A benchmark for data
visualization in the era of large language models.
IEEE Transactions on Visualization and Computer
Graphics, 31(1):1301–1311.
David Zhang. 2025.
https://github.com/dzhng/deep-
research.
https://github.com/dzhng/
deep-research.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam
Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023.
Mind2web: Towards a generalist agent for the web.
In Advances in Neural Information Processing Sys-
tems, volume 36, pages 28091–28114. Curran Asso-
ciates, Inc.
Victor Dibia. 2023. LIDA: A tool for automatic gen-
eration of grammar-agnostic visualizations and info-
graphics using large language models. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System
Demonstrations), pages 113–126, Toronto, Canada.
Association for Computational Linguistics.
Google. 2024. Gemini deep research.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning.
arXiv preprint
arXiv:2501.12948.
Yucheng Han, Chi Zhang, Xin Chen, Xu Yang,
Zhibin Wang, Gang Yu, Bin Fu, and Hanwang
Zhang. 2023. Chartllama: A multimodal llm for
chart understanding and generation. arXiv preprint
arXiv:2311.16483.
Liu He, Yizhi Song, Hejun Huang, Pinxin Liu, Yunlong
Tang, Daniel Aliaga, and Xin Zhou. 2025. Kubrick:
Multimodal agent collaborations for synthetic video
generation. Preprint, arXiv:2408.10453.
Yi He, Ke Xu, Shixiong Cao, Yang Shi, Qing Chen,
and Nan Cao. 2024. Leveraging foundation models
for crafting narrative visualization: A survey. IEEE
transactions on visualization and computer graphics.
9
Jeffrey Heer and Michael Bostock. 2010. Declarative
language design for interactive visualization. IEEE
Transactions on Visualization and Computer Graph-
ics, 16(6):1149–1156.
Matt-Heun Hong and Anamaria Crisan. 2023. Conver-
sational ai threads for visualizing multidimensional
datasets. Preprint, arXiv:2311.05590.
Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran
Hao, Liuyihan Song, Yang Xu, J Yang, JH Liu,
Chenchen Zhang, Linzheng Chai, and 1 others.
2024.
Opencoder: The open cookbook for top-
tier code large language models.
arXiv preprint
arXiv:2411.04905.
Fantine Huot, Reinald Kim Amplayo, Jennimaria Palo-
maki, Alice Shoshana Jakobovits, Elizabeth Clark,
and Mirella Lapata. 2025. Agents’ room: Narrative
generation through multi-step collaboration. In Inter-
national Conference on Learning Representations.
Mohammed Saidul Islam, Md Tahmid Rahman Laskar,
Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty.
2024. DataNarrative: Automated data-driven story-
telling with visualizations and texts. In Proceedings
of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 19253–19286,
Miami, Florida, USA. Association for Computational
Linguistics.
Carlos E Jimenez, John Yang, Alexander Wettig,
Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R
Narasimhan. 2024. SWE-bench: Can language mod-
els resolve real-world github issues? In The Twelfth
International Conference on Learning Representa-
tions.
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon,
Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei
Han. 2025. Search-r1: Training llms to reason and
leverage search engines with reinforcement learning.
arXiv preprint arXiv:2503.09516.
Max Ku, Thomas Chong, Jonathan Leung, Krish
Shah, Alvin Yu, and Wenhu Chen. 2025.
The-
oremexplainagent: Towards multimodal explana-
tions for llm theorem understanding. arXiv preprint
arXiv:2502.19400.
Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and
Wenhu Chen. 2024. VIEScore: Towards explainable
metrics for conditional image synthesis evaluation.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 12268–12290, Bangkok, Thai-
land. Association for Computational Linguistics.
Guozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan
Zheng, Yu Zhang, Chuangxin Ou, Song Wang, and
Chi Harold Liu. 2024a.
Visualization generation
with large language models: An evaluation. Preprint,
arXiv:2401.11255.
Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya
Du. 2024b. Mlr-copilot: Autonomous machine learn-
ing research based on large language models agents.
Preprint, arXiv:2408.14033.
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,
Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng
Dou. 2025a. Search-o1: Agentic search-enhanced
large reasoning models. CoRR, abs/2501.05366.
Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yu-
tao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng
Dou. 2025b. Webthinker: Empowering large rea-
soning models with deep research capability. CoRR,
abs/2504.21776.
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian
Zhang, Yutao Zhu, and Zhicheng Dou. 2025c. From
matching to generation: A survey on generative infor-
mation retrieval. ACM Transactions on Information
Systems, 43(3):1–62.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
NLG evaluation using gpt-4 with better human align-
ment. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 2511–2522, Singapore. Association for Com-
putational Linguistics.
Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-
ster, Jeff Clune, and David Ha. 2024. The AI Scien-
tist: Towards fully automated open-ended scientific
discovery. arXiv preprint arXiv:2408.06292.
Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Jun-
wei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue
Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei
Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu
Liu, Jingyang Yuan, Shichang Zhang, and 7 others.
2025. Large language model agent: A survey on
methodology, applications and challenges. Preprint,
arXiv:2503.21460.
Paula Maddigan and Teo Susnjak. 2023. Chat2vis: Fine-
tuning data visualisations using multilingual natural
language text and pre-trained large language models.
Preprint, arXiv:2303.14292.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
and 1 others. 2021.
Webgpt: Browser-assisted
question-answering with human feedback.
arXiv
preprint arXiv:2112.09332.
Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-
vio Savarese, and Yingbo Zhou. 2023a. Codegen2:
Lessons for training llms on programming and natu-
ral languages. ICLR.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2023b. Codegen: An open large language
model for code with multi-turn program synthesis.
ICLR.
10
2025-12-10 16:48:39 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:126 | The title of the paper is Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework

Here's first ten page content: arXiv:2506.02454v1  [cs.CL]  3 Jun 2025
Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
From Scratch with Agentic Framework
Zhaorui Yang§*, Bo Pan§*, Han Wang§*, Yiyao Wang§, Xingyu Liu§
Minfeng Zhu‡B, Bo Zhang§B, Wei Chen§
§State Key Lab of CAD&CG, Zhejiang University
‡Zhejiang University
https://rickyang1114.github.io/multimodal-deepresearcher/
Abstract
Visualizations play a crucial part in effective
communication of concepts and information.
Recent advances in reasoning and retrieval aug-
mented generation have enabled Large Lan-
guage Models (LLMs) to perform deep re-
search and generate comprehensive reports. De-
spite its progress, existing deep research frame-
works primarily focus on generating text-only
content, leaving the automated generation of
interleaved texts and visualizations underex-
plored. This novel task poses key challenges
in designing informative visualizations and ef-
fectively integrating them with text reports. To
address these challenges, we propose Formal
Description of Visualization (FDV), a struc-
tured textual representation of charts that en-
ables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this
representation, we introduce Multimodal Deep-
Researcher, an agentic framework that decom-
poses the task into four stages: (1) researching,
(2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the
evaluation of generated multimodal reports, we
develop MultimodalReportBench, which con-
tains 100 diverse topics served as inputs along
with 5 dedicated metrics. Extensive experi-
ments across models and evaluation methods
demonstrate the effectiveness of Multimodal
DeepResearcher. Notably, utilizing the same
Claude 3.7 Sonnet model, Multimodal DeepRe-
searcher achieves an 82% overall win rate over
the baseline method.
1
Introduction
Large language models (LLMs) have demonstrated
broad capabilities in solving diverse tasks such as
question answering, coding and math (Bai et al.,
2022; Guo et al., 2025; Huang et al., 2024). Aug-
mented with searching and reasoning capabili-
ties (Xie et al., 2023; Nakano et al., 2021; Li et al.,
*Equal Contribution
BCorresponding Authors
Under review. Code will be released upon acceptance.
This remarkable pace of activity resulted in a record 2,664 cataloged objects 
entering Earth's orbit—surpassing any previous year in spaceflight history. [...]
Figure 1: A text-chart interleaved snippet from the re-
port generated by our Multimodal DeepResearcher.
2025a), LLMs can perform deep research and effec-
tively leverage up-to-date external information be-
yond static parameters (Li et al., 2025a). Recently,
this paradigm has garnered significant attention
with its remarkable efficacy in generating grounded,
comprehensive reports from scratch (Shao et al.,
2024; Huot et al., 2025). However, existing deep
research frameworks from both academia (Jin et al.,
2025; Zheng et al., 2025b; Li et al., 2025b) and in-
dustry (OpenAI, 2025c; Google, 2024; xAI, 2025;
David Zhang, 2025) predominantly focus on gen-
erating text-only content, neglecting the display
beyond text modality. The text-heavy nature of
these reports impedes effective communication of
concepts and information (Ku et al., 2025; Zheng
et al., 2025a), which limits their readability and
practical utility.
In real-world scenarios, visualization serves as
a crucial part of reports and presentations, offer-
ing remarkable capabilities for conveying data in-
sights (Otten et al., 2015), facilitating the identifica-
tion of implicit patterns (Yang et al., 2024), and en-
hancing audience engagement (Barrick et al., 2018;
Zheng et al., 2025a). Human experts typically craft
1
meticulously designed visualizations with consis-
tent styles to effectively communicate ideas and
insights. They then integrate these visualizations
within appropriate textual context (He et al., 2024)
to create coherent text-chart interleaved reports.
However, the end-to-end generation of multi-
modal reports remains challenging.
Although
prompting LLMs to generate individual visualiza-
tion charts is a promising solution (Yang et al.,
2024; Seo et al., 2025; Han et al., 2023), effec-
tively representing and integrating these visualiza-
tions with textual content poses significant chal-
lenges. Although in-context learning appears to be
a promising approach for guiding such generation,
the absence of a standardized format for text-chart
interleaved content impedes effective implementa-
tion of in-context learning strategies.
To address this challenge, we introduce the For-
mal Description of Visualization (FDV), a struc-
tured representation method inspired by the gram-
mar of graphics theory (Wilkinson, 1999). FDV
comprehensively captures visualization designs
through four perspectives (i.e., overall layout, plot-
ting scale, data, and marks). This representation
provides universal and high-fidelity descriptions
that enables in-context learning of human expert
designs and produce charts of professional quality.
Building upon FDV, we introduce Multimodal
DeepResearcher, an agentic framework that gener-
ates text-chart interleaved reports from scratch. A
snippet of generated report is illustrated in Figure 1.
The framework operates through four stages: (1)
researching, which gathers comprehensive infor-
mation through searching and reasoning; (2) exem-
plar report textualization, which textualizes multi-
modal reports from human experts using our pro-
posed Formal Description of Visualization (FDV,
Section 3.2) for in-context learning; (3) planning,
which establishes a content outline and visualiza-
tion style guide to ensure consistency throughout
the report; and (4) multimodal report generation,
which produces the final interleaved report through
drafting, coding and iterative chart refinement.
We evaluate our framework with Multimodal-
Bench (Section 4.1), which comprises 100 topics
used as inputs and 5 dedicated evaluation metrics.
Our experiments include both proprietary and open-
source models with automatic and human evalua-
tion. As a baseline, we adapted DataNarrative (Is-
lam et al., 2024), a relevant framework that gener-
ates placeholders for charts from tabular inputs, to
perform our task. Both automatic and human evalu-
ations consistently demonstrate Multimodal Deep-
Researcher’s superior performance compared to
the baseline. Notably, when using Claude 3.7 Son-
net as the generator, Multimodal DeepResearcher
achieves an impressive 82% overall win rate.
Our contributions can be summarized as follows:
• We propose a novel task that generates a
text-chart interleaved multimodal report from
scratch and a corresponding dataset and eval-
uation metrics.
• We propose Formal Description of Visualiza-
tion, a structured textual representation of vi-
sualizations that enables the in-context learn-
ing and generation of multimodal reports.
• We introduce Multimodal DeepResearcher,
an end-to-end agentic framework that gener-
ates high-quality multimodal reports, which
largely outperform the baseline method.
2
Related Work
Deep Research
Recently, the combination of re-
trieval techniques (Li et al., 2025c; Zhao et al.,
2024) and reasoning (Guo et al., 2025) has enabled
LLMs to transcend their parametric constraints by
leveraging external knowledge. Pioneering works
have designed specialized prompts and workflows
for complex research tasks, as exemplified by Open-
Researcher (Zheng et al., 2024) and Search-o1 (Li
et al., 2025a). Subsequent research has explored
reinforcement learning for end-to-end reasoning
and information retrieval (Jin et al., 2025; Zheng
et al., 2025b). However, these approaches primar-
ily focus on generating and evaluating text-only
results, whereas our study advances the field by
generating text-chart interleaved reports that sig-
nificantly enhance information comprehension and
communication with visualizations.
LLM for Data Visualizations
Current work
has focused on enhancing individual chart quality
through various approaches, including multi-stage
pipelines (Dibia, 2023), iterative debugging with vi-
sual feedback (Yang et al., 2024), chain-of-thought
prompted query reformulation (Seo et al., 2025),
and models fine-tuned with domain-specific data
for chart generation (Han et al., 2023; Tian et al.,
2024). Other research has explored how to articu-
late generation intent, such as multimodal prompt-
ing with sketches and direct manipulations (Wen
2
“Tell me 
about xxx.”
Knowledge
Exemplar
Reports
FDV
Outline
Visualization
Style Guide
Researching A
Exemplar Textualization B
Report Generation D
Final Report
Layout Scale
Data
Marks
Planning C
Textual Report
Text
Information
&
References
Drafting
Coding
In-context
Learning
MLLM
Refining
Figure 2: The framework of the Multimodal DeepResearcher. It decomposes the task of multimodal report
generation into four stages: A Iterative researching about given topic; B Exemplar textualization of human experts
using proposed Formal Description of Visualization (FDV, Section 3.2); C Planning; D Report Generation, which
generates the final report with crafting, coding and iterative refinement.
et al., 2025), multilingual natural language inter-
faces (Maddigan and Susnjak, 2023), and conver-
sational context management (Hong and Crisan,
2023). Corresponding evaluation methodologies
have also been proposed (Li et al., 2024a; Chen
et al., 2025). Unlike previous work that focuses
predominantly on single chart or limited data and
chart types, our work is the first to generate and
evaluate text-chart interleaved reports, which con-
tains multiple diverse visualizations based on in-
the-wild complex and heterogeneous information.
LLM for agentic generation
LLMs have been
widely applied to various generation tasks due to
their ability to process complex textual informa-
tion (Ku et al., 2024; Nijkamp et al., 2023b,a;
Jimenez et al., 2024; Yang et al., 2025b). For more
challenging tasks, researchers have designed LLM
agents that decompose problems into reasoning,
planning, and execution stages (Luo et al., 2025).
These agents have demonstrated remarkable suc-
cess across scientific research (Lu et al., 2024; Si
et al., 2024; Li et al., 2024b; Bogin et al., 2024),
video generation (He et al., 2025), and computer
system interaction (Xie et al., 2024; Deng et al.,
2023; Zhang et al., 2023). This paradigm extends
effectively to the visualization domain as well. The-
oremExplainAgent (Ku et al., 2025) uses agents to
generate educational videos, and PPTAgent (Zheng
et al., 2025a) automatically creates presentations in
the form of slides with integrated text and visuals.
Most relevant to our work, DataNarrative (Islam
et al., 2024) explores generating simple specifica-
tions for data-driven visualizations and evaluating
these specifications as proxies for visualization as-
sessment. However, this approach remains limited
to simple chart types (e.g., bar chart or line chart),
which restricts their practical use.
3
Method
We formulate the task of multimodal report genera-
tion as follows: given a topic t and a set of multi-
modal exemplar reports R containing interleaved
textual content and charts, the system is expected to
output a multimodal report as in R based on t. To
solve this task, we introduce Multimodal DeepRe-
searcher, an agentic framework which decomposes
the task into four steps: (1) researching through it-
erative web search and reasoning (Section 3.1); (2)
exemplar report textualization (Section 3.2), which
textualizes multimodal exemplar reports from hu-
man experts using our proposed Formal Descrip-
tion of Visualization (FDV, Section 3.2); (3) plan-
ning (Section 3.3); and (4) Multimodal report gen-
eration (Section 3.4). An overview of Multimodal
DeepResearcher is presented in Figure 2.
3.1
Researching
To leverage up-to-date information beyond para-
metric knowledge, Multimodal DeepResearcher
conducts iterative research on a given topic t, gen-
erating a comprehensive set of learnings L. These
learnings encompass both information acquired
through web sources and their corresponding ref-
erences. The process involves iterative execution
of two primary operations: (1) web search and
(2) subsequent reasoning based on search results.
Initially, the agent prompts the LLM to generate
relevant keywords K = k1, · · · , knK based on
the given topic t. The agent then conducts web
3
- The visualization consists of two similar strip plots stacked vertically. 
- Each plot has a title at the top ('UK City Traffic Volume' and 'US City Traffic Volume'). 
- The overall chart has a shared legend at the bottom showing 'UK City Mean' and 'US City 
Mean' with corresponding colored lines. 
- Each plot has adequate margins on all sides, with city names aligned on the left side.
- X-axis: Linear scale from 0 to 9, representing 'Vehicles per hour (thousands)'. 
- X-axis has grid lines at 1-unit intervals (1, 2, 3, etc.). 
- X-axis label 'Vehicles per hour (thousands)' is placed at the bottom of each plot. 
- Y-axis: Categorical scale showing city names, evenly spaced vertically. 
- No y-axis title is shown, just the city names as tick labels aligned to the left.
- Color: All marks shown in grey by default, except that values above the UK mean for UK 
cities are marked in red, and values above the US mean for US cities are marked in blue.
- For each city, there are multiple traffic volume measurements, represented as small marks. 
- The mean traffic volume for each country (UK and US) is calculated and visualized as 
vertical lines.
- Small tick marks (resembling small vertical lines) represent individual traffic volume 
measurements for each city, with colors indicating both the country and whether values are 
above or below the mean. 
- A vertical red line represents the UK mean traffic volume in the top plot. 
- A vertical blue line represents the US mean traffic volume in the bottom plot.
Extract
Design
 
Implement
Design
(B) Formal Description of Visualization 
(A) Origin Visualization
(C) Reconstructed Visualization
Figure 3: The illustration Formal Description of Visualization (FDV) for the exemplar textualization process.
(A) Original traffic volume visualizations for UK and US cities; (B) The Formal Description of Visualization
(FDV) that systematically captures the visualization’s layout, scale, data, and marks using a structured format;
and (C) The reconstructed visualization based on the formal description. This process textualizes high-quality
text-chart interleaved reports by transforming visual elements into structured textual representations that preserve
the visualization’s essential characteristics.
searches using these keywords and retrieves web-
pages P = p1, · · · , pnP . Subsequently, the agent
analyzes these webpages, synthesizes the informa-
tion into learnings L, and formulates a research
question q for the next iteration. Based on this re-
search question and the original topic, the research
agent performs the next research cycle. After nR
rounds of iteration, the researcher produces a final
compilation of learnings. Further details of this
process are provided in Appendix A.1.
3.2
Exemplar Textualization
Human experts typically produce reports with both
texts and visualizations to enhance communication
and audience engagement (Zheng et al., 2025a;
Yang et al., 2024). To generate high-quality mul-
timodal content comparable to expert-created re-
ports, we employ in-context learning with exemplar
reports crafted by human experts. This approach
necessitates an effective methodology for convert-
ing multimodal exemplar reports R into textual
exemplar reports ˜R.
To address this challenge, we propose Formal
Description of Visualization (FDV), a structured
description method for visualization charts inspired
by the grammar of graphics (GoG) theory (Wilkin-
son, 1999), which theoretically provides universal
and high-fidelity descriptions for any visualization
designs. As shown in Figure 3 (B), FDV charac-
terizes each visualization chart from four perspec-
tives: (1) Overall layout, detailing the constituent
subplots and their spatial arrangements; (2) Plot-
ting scale, describing the scaling logic behind each
“data to visual channel (e.g., position, color)” map-
ping and their annotations; (3) Data, describing
both the numeric data and text elements used to gen-
erate the visualization. (4) Marks, describing the
design specifications of each visual element. The
reverse process of textualization can be achieved
via coding, which reconstructs the visualization
from FDV, as shown in Figure 3 (C).
In the practical implementation of textualization,
Multimodal DeepResearcher first extracts all vi-
sualization charts from the report, then prompts a
multimodal large language model to textualize and
replace each of them. This process is presented in
Algorithm 1. The full prompt for the textualization
process is provided in appendix B.2.
3.3
Planning
After iterative researching about the topic t through
nR rounds, Multimodal DeepResearcher creates a
plan before the actual generation process. Specif-
ically, it constructs an outline O that establishes
the narrative architecture of the report based on the
learnings L, topic t and textual exemplar report ˜R.
The outline comprises a hierarchical structure of
sections, each containing a descriptive title and a
brief summary. To learn the style of visualizations
present in exemplar reports ˜R and maintain a con-
sistent style in the generation of each visualization
chart, Multimodal DeepResearcher also prompts
the LLM to generate a visualization style guide G.
The visualization style guide provides guidelines
4
Algorithm 1 Textualization of multimodal reports
1: Inputs: Multimodal exemplar reports R.
2: Requires: Multimodal large language model
Mv, replace function replace.
3: Outputs: Textualized exemplar reports ˜R.
4: Initialize ˜R = ∅
5: for r in R do
6:
Init. ˜r = r
7:
for each image i in r do
8:
// Extract FDV from image
9:
FDVi = Mv(i)
10:
// Replace image with extracted FDV
11:
˜r = ˜r.replace(i, FDVi)
12:
end for
13:
˜R = ˜R ∪{˜r}
14: end for
15: Return: ˜R
that control the overall style of visualizations in the
report (e.g., color palette, font hierarchy). More
details of this process can be found in appendix A.
3.4
Generating the Final Report
The final stage of Multimodal DeepResearcher is
responsible for the actual generation of the multi-
modal report with interleaved textual content and
visualizations. The report is generated with out-
puts of previous stages, i.e., learnings L, exemplar
textual reports ˜R, outline O and visualization style
guide G.
Multimodal DeepResearcher first prompts the
LLM to generate a textual report with Formal De-
scription of Visualization (FDV) defined in Sec-
tion 3.2 as a placeholder for the underlying visu-
alization chart to be generated. The format of this
textual report is expected to be the same as those in
textual exemplar reports used for in-context learn-
ing. Then, Multimodal DeepResearcher extracts
all occurrences of FDVs, and prompts the LLM to
implement the design via coding. Since visualiza-
tions represented by FDV have extensive flexibility,
which may exceed the expressive capabilities of
typical declarative visualization libraries (Heer and
Bostock, 2010) (e.g., matplotlib), we directed the
LLMs to utilize D3 , the most widely used im-
perative visualization programming library based
on JavaScript and HTML, to implement the target
visualization designs.
To further improve the quality of visualizations
generated, Multimodal DeepResearcher includes
https://d3js.org
an actor-critic mechanism to revise the code for
generating the visualization charts motivated by
recent advancements of agents (Yang et al., 2024).
In this scenario, the actor is the LLM Mt responsi-
ble for generating the code, and the critic feedback
comes from both the console and a critic model.
Console feedback is collected using chrome de-
veloper tools provided as Python package to sim-
ulate a browser. It first collects console messages
that may contain errors during the loading of visu-
alizations. After all elements are loaded, it takes a
screenshot to obtain the visualization chart.
After getting the screenshot of the visualization
chart, Multimodal DeepResearcher employs a mul-
timodal LLM Mv to serve as a critic, which pro-
vides visual feedback. The multimodal LLM takes
the chart rendered as input, examines the visual
quality, and delivers corresponding feedback. It
further determines whether the current chart needs
improvement. If improvement is needed, the actor
refines its code based on the feedback and console
message. This iterative refinement continues until
the critic is satisfied, or the predefined upper limit
of retry times is reached, which we set as 3 to avoid
infinite refinement cycles. When the refinement
process finishes, the critic selects the final chart
from the final two iterations.
The algorithm for the refine process is presented
in Algorithm 2. The prompts employed during
this process are detailed in appendix B.5. A com-
prehensive full report generated by Multimodal
DeepResearcher is presented in Appendix E.
4
Experiments
In this section, we present the MultimodalReport-
Bench and corresponding evaluation criteria for
evaluation, followed by the experimental results.
4.1
Data Selection
To systematically evaluate the multimodal report
generated by Multimodal DeepResearcher, we con-
structed a dataset comprising 100 real-world topics
curated from public websites that feature multi-
modal reports crafted by human experts, i.e., Pew
Research (Pew, 2025), Our World in Data (OWID,
2025) and Open Knowledge Foundation (OKF,
2024). Pew Research informs the public about is-
sues, attitudes and trends shaping the world through
research report. Our World in Data presents em-
pirical data and research on global development
https://developer.chrome.com/docs/devtools
5
Algorithm 2 Algorithm for refining charts
1: Inputs: chart c represented as code.
2: Requires: Browser tool T, LLM Mt, Multi-
modal LLM Mv.
3: Outputs: Refined chart ˜c.
4: Hypars: Number of max retry times Nmax.
5: Initialize satisfied = False, c0 = c, C = {c}.
6: for i = 1 to Nmax do
7:
// Get console message and image
8:
msg, i = T(c)
9:
// Critic Mv evaluates the chart
10:
satisfied, feedback = Mv(i)
11:
if satisfied == True then
12:
break
13:
end if
14:
// actor Mt refines previous chart
15:
ci = Mt(ci−1, msg, feedback)
16:
C = C ∪{ci}
17: end for
18: ˜c = c0
19: if |C| > 1 then
20:
// Selects from the last two charts
21:
˜c = Mv(C[−1], C[−2])
22: end if
23: Return: ˜c
challenges through web publications. The Open
Knowledge Foundation is dedicated to promoting
open data and content across all domains, ensuring
information accessibility. These sources contain
exemplary multimodal reports, making their topics
appropriate for our evaluation task.
The topics are then used as inputs for multimodal
report generation. To ensure that our dataset applies
to the real-world scenario, we meticulously curated
topics spanning 10 categories, such as travel, en-
ergy and education. Table 4 presents the distri-
bution of topics. We also collected 6 multimodal
reports with no overlapping in topics to serve as ex-
emplar reports for in-context learning, as described
in Section 3.2.
4.2
Baseline Selection
Our task requires generating a multimodal re-
port from scratch, which is infeasible with direct
prompting or existing deep research frameworks.
Most similar to our work, DataNarrative (Islam
et al., 2024) generates simple data-driven visual-
ization specifications based on data tables as input,
and evaluates the textual specification as a proxy of
chart. We incorporate our researching module and
adapt its framework accordingly to establish our
baseline. For an apple-to-apple comparison, we
utilize the learnings generated with our researching
stage (Section 3.1) and plans (Section 3.3) instead
of tables as the input. It then goes through generate-
verify-refine process, consistent with the original
framework. Since the original framework lacks
mechanisms for transforming design specifications
into actual charts, we extract all design specifi-
cations and generate corresponding visualizations
using the same pipeline as Multimodal Researcher
does in Section 3.4.
4.3
Framework Implementation
Multimodal DeepResearcher is an agentic frame-
work with multiple stages. In this section, we de-
scribe the implementation details of each stage.
In the researching stage (Section 3.1), we perform
web search and scrape with Firecrawl API, and con-
duct reasoning with GPT-4o-mini (OpenAI, 2025a).
GPT-4o-mini is also utilized for planning. Claude
3.7 Sonnet (Anthropic, 2025) is utilized as the
MLLM for the textualization of exemplar reports
(Section 3.2). The generation of the final multi-
modal report requires both a large language model
to craft textual report, and a multimodal large lan-
guage model to provide visual feedback for the
chart. Our experiments encompasses two configura-
tions: (1) State-of-the-art proprietary models, with
Claude 3.7 Sonnet serving as both the LLM and
multimodal LLM. (2) Open-source models, specifi-
cally Qwen3-235B-A22B (Yang et al., 2025a) and
Qwen2.5-VL-72B-Instruct (Bai et al., 2025). To
ensure fair comparison, all the settings are consis-
tent in both Multimodal DeepResearcher and the
DataNarrative baseline where applicable. All calls
were made from OpenRouter with no GPU utilized
in our experiments.
4.4
Automatic Evaluation
Given the multimodal nature of the outputs in our
task, evaluation necessitates assessment of both
texts and visualizations. To accomplish this, we
convert the visualizations generated into base64
encoding, and prompt a Multimodal LLM to con-
duct head-to-head comparisons of two reports with
the format of OpenAI messages. Specifically, we
utilized GPT-4.1 (OpenAI, 2025b) as the evalua-
https://www.firecrawl.dev/
https://openrouter.ai/
https://platform.openai.com/docs/
api-reference/images
6
tor in all automatic evaluation experiments. Since
report generation constitutes an open-ended, objec-
tive task, reference-based metrics typically fail to
align with human-perceived standards (Liu et al.,
2023). Therefore, we established a comprehensive
criteria incorporating both texts and visualizations
in reports, which primarily consists of five metrics:
Informativeness and Depth. Evaluates whether
the report delivers comprehensive, substantive and
thorough information through both texts and ac-
company visualizations.
Coherence and Organization.
Evaluates
whether the report is well-organized, and whether
the visualizations connect meaningfully to the text.
Verifiability. Evaluates whether the information
of the reports can be verified with citations. Apart
from textual links to references, we also prompt
the evaluator to check the annotation present in
visualizations that may contain source information.
Visualization Quality. Evaluates the quality of
visualization charts in the report, including visual
clarity and textual labels and annotations.
Visualization Consistency. Evaluates whether
the visualizations in the report maintain a consistent
overall style. The style contains the color palettes,
typography and information hierarchy in visualiza-
tions.
During evaluation, we provide the evaluator with
the topic, learnings which contain both knowledge
acquired through web search and corresponding ref-
erences and both reports. Specifically, we prompt
the evaluator to rate both reports between on a 1-5
scale with detailed guides, subsequently comparing
scores to determine superiority or equivalence. To
mitigate positional bias, we randomize the presen-
tation order of reports. The complete evaluation
prompt is provided at appendix B.6.
Results. As illustrated in Table 1, Multimodal
DeepResearcher consistently outperforms Data-
Narrativeacross across both proprietary and open-
source model configurations. With Claude 3.7 Son-
net, it achieves an overall win rate of 82%. Specif-
ically, Multimodal DeepResearcher outperforms
with a high win rate in Verifiability (86%), Vi-
sualization Quality (80%) and Visualization con-
sistency (78%).
A similar pattern is observed
with open-source models (Qwen3-235B-A22B and
Qwen2.5-VL-72B-Instruct), where Multimodal
DeepResearcher achieves a win rate of 55%. The
results demonstrate the efficacy of Multimodal
DeepResearcher in generating multimodal reports.
Table 1: Automatic evaluation results of the multimodal
report: Multimodal DeepResearcher (Ours) vs. Data-
Narrative.
Ours vs DataNarrative
Evaluation Metrics
Ours Win
Ours Lose
Tie
w. Claude 3.7 Sonnet
Informativeness and Depth
75%
25%
0%
Coherence and Organization
76%
21%
3%
Verifiability
86%
5%
9%
Visualization Quality
80%
16%
4%
Visualization Consistency
78%
17%
5%
Overall
82%
16%
2%
w. Qwen3-235B-A22B & Qwen2.5-VL-72B-Instruct
Informativeness and Depth
50%
50%
0%
Coherence and Organization
41%
51%
8%
Verifiability
66%
21%
13%
Visualization Quality
48%
46%
6%
Visualization Consistency
52%
42%
6%
Overall
55%
40%
5%
Table 2: Human evaluation of the generated reports:
Multimodal DeepResearcher (Ours) vs. DataNarrative.
Evaluation Metrics
Ours Win
Ours Lose
Tie
Informativeness and Depth
100%
0%
0%
Coherence and Organization
100%
0%
0%
Verifiability
100%
0%
0%
Visualization Quality
80%
10%
10%
Visualization Consistency
80%
10%
10%
Overall
100%
0%
0%
4.5
Human Evaluation
For human evaluation, we utilized the same set of
metrics as in automatic evaluation. We selected a
random subset of 10 topics for evaluation. Specifi-
cally, 3 annotators performed pairwise comparison
of reports generated by both Multimodal DeepRe-
searcher and DataNarrative with Claude 3.7 Son-
net. As with automatic evaluation (Section 4.5),
we randomized report presentation order to avoid
positional bias. Results are presented in Table 2.
Surprisingly, Multimodal DeepResearcher achieves
an overall win rate of 100%. Specifically, two an-
notators preferred all 10 reports generated by Mul-
timodal DeepResearcher, while the third annotator
preferred 9 out of 10. The results further validate
the effectiveness of Multimodal Deepresearcher.
4.6
Ablation Studies
To assess the efficacy of individual components of
Multimodal DeepResearcher, we conducted abla-
tion experiments on a random subset of 20 topics.
Specifically, we compared 3 variants against Multi-
7
Table 3: Results of ablation studies across three differ-
ent setups. We report the lose, win and tie rates for
each setup against the complete framework. Claude 3.7
Sonnet serves as both the LLM and MLLM here.
Ablated Components
Lose
Win
Tie
- w/o Exemplar Learning
70%
20%
10%
- w/o Planning
85%
15%
0%
- w/o Refinement of charts
80%
20%
0%
modal DeepResearcher: (1) w/o in-context learning
from exemplar reports (Section 3.2); (2) w/o plan-
ning (Section 3.3); (3) w/o iterative refinement of
charts (Section 3.4). To ensure fair comparison,
all other settings and hyperparameters remained
consistent across variants. As shown in table 3,
removing any component results in significant per-
formance degradation. Specifically, eliminating ex-
emplar learning from human reports yields a 70%
lose rate, direct generation without planning leads
to 85% lose rate, and removing chart refinement
process loses in 80% cases. These findings demon-
strate the contribution of each component to the
effectiveness of Multimodal DeepResearcher.
5
Analysis
5.1
Visualization Analysis
In this section, we analyze the characteristics of
visualizations generated with Multimodal Deep-
Researcher and the baseline. While the average
number of charts per report between our frame-
work (9.3) and DataNarrative (9.4) is comparable,
the visualizations generated by Multimodal Deep-
Researcher are notably more diverse. As illustrated
in Figure 4, although both methods prioritized con-
ventional chart types such as bar charts and line
charts, Multimodal DeepResearcher demonstrates
superior capability in generating sophisticated and
complex visualizations.
For instance, across the 100 selected topics, Mul-
timodal DeepResearcher produces 15 flowcharts
and 18 dashboards, while DataNarrative generates
merely 2 flowcharts and 1 dashboard. Another
example involves the “Others” category, which en-
compasses hard-to-categorize visualizations such
as infographics and mind maps. Our framework
generates 280 such charts, substantially exceeding
the 96 produced by DataNarrative. This disparity
underscores our approach’s flexibility in accommo-
dating to diverse real-world scenarios. We provide
a collection of examples for each type generated
403 (43.9%)
262 (28.5%)
53 (5.8%)
54 (5.9%)
96 (10.4%)
DataNarrative
282 (31.1%)
209 (23.0%)
280 (30.9%)
Multimodal DeepResearcher
Bar chart
Line chart
Pie chart
Scatter chart
Bubble chart
Flowchart
Dashboard
Choropleth map
Sankey diagram
Others
Figure 4: Distribution of visualization charts generated
with DataNarrative and Multimodal DeepResearcher
(Ours). The first column in the legend (denoted by red
and yellow colors) represents conventional chart types.
by Multimodal DeepResearcher in appendix C.
5.2
Error Analysis
Despite the remarkable efficacy of Multimodal
DeepResearcher, the integration of visualizations
poses new challenges. In this section, we catego-
rize the identified common errors into the following
two categories.
Overlapping
Overlapping of elements is the
most common error. It is generally attributed to two
factors: (1) excessive information in FDV that com-
plicates proper arrangement within limited space.
(2) suboptimal placement of legends, labels and an-
notations. Illustrative examples of both scenarios
are provided in Appendix D.
Hallucination
Hallucination persists as a funda-
mental challenge for LLMs (Shao et al., 2024; Is-
lam et al., 2024), which also extends to the gener-
ation of visualizations. Figure 17 exemplifies this
issue through a choropleth map example, where the
model erroneously marked regions with inadequate
data with hallucinated content using red color.
6
Conclusion
In this work, we investigate the challenge of gen-
erating multimodal reports from scratch. We in-
troduce the Formal Description of Visualization,
a structured representation of charts that enables
in-context learning from human-created exemplar
reports. Based on this, we propose Multimodal
DeepResearcher, an end-to-end framework for the
generation of multimodal reports. While exten-
sive experiments with both automatic evaluation
and human evaluation confirm the efficacy of our
8
framework, challenges remain in improving visual-
ization quality and reducing hallucination.
Limitations
Although Multimodal DeepResearcher has demon-
strated remarkable potential in end-to-end genera-
tion of multimodal reports from scratch, the frame-
work contains limitations due to the complex nature
of the task. First, several types of errors exist in
the generated visualizations, as discussed in Sec-
tion 5.2. Furthermore, in-context learning from
exemplar reports imposes demands on context size
and understanding capabilities of LLMs. Moreover,
the considerable computational expenditure associ-
ated with state-of-the-art models, coupled with the
extensive processing time required for visualization
code generation, necessitated the utilization of a
relatively constrained dataset for experimental vali-
dation. The totality of these experiments incurred
approximately 650 USD in API usage fees.
Ethical Considerations
For human evaluation, we recruited three partici-
pants with extensive academic paper writing expe-
rience from a local university. Prior to the experi-
ment, participants were informed about the study
duration and general procedures, and consent to
participate. After human evaluation, all partici-
pants were compensated with rates above average
wage.
We also recognize and uphold the importance of
intellectual property. The data used by our Multi-
modal DeepResearcher system was obtained solely
from publicly accessible and legally permissible
sources, where academic utilization is approved.
All the selected topics are carefully checked to ex-
clude potentially offensive content.
We used large language models as auxiliary tools
to facilitate the writing of the manuscript, with
careful verification to ensure precision.
References
Anthropic. 2025. Claude. https://www.anthropic.
com/claude/sonnet.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-
bin Ge, Sibo Song, Kai Dang, Peng Wang, Shi-
jie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu,
Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei
Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others.
2025. Qwen2.5-vl technical report. arXiv preprint
arXiv:2502.13923.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, and 1
others. 2022. Training a helpful and harmless assis-
tant with reinforcement learning from human feed-
back. arXiv preprint arXiv:2204.05862.
Andrea Barrick, Dana Davis, and Dana Winkler. 2018.
Image versus text in powerpoint lectures: Who does
it benefit? Journal of Baccalaureate Social Work,
23(1):91–109.
Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle
Richardson, Erin Bransom, Peter Clark, Ashish Sab-
harwal, and Tushar Khot. 2024. Super: Evaluating
agents on setting up and executing tasks from re-
search repositories. Preprint, arXiv:2409.07440.
Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, and
Yuqing Yang. 2025. Viseval: A benchmark for data
visualization in the era of large language models.
IEEE Transactions on Visualization and Computer
Graphics, 31(1):1301–1311.
David Zhang. 2025.
https://github.com/dzhng/deep-
research.
https://github.com/dzhng/
deep-research.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam
Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023.
Mind2web: Towards a generalist agent for the web.
In Advances in Neural Information Processing Sys-
tems, volume 36, pages 28091–28114. Curran Asso-
ciates, Inc.
Victor Dibia. 2023. LIDA: A tool for automatic gen-
eration of grammar-agnostic visualizations and info-
graphics using large language models. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System
Demonstrations), pages 113–126, Toronto, Canada.
Association for Computational Linguistics.
Google. 2024. Gemini deep research.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning.
arXiv preprint
arXiv:2501.12948.
Yucheng Han, Chi Zhang, Xin Chen, Xu Yang,
Zhibin Wang, Gang Yu, Bin Fu, and Hanwang
Zhang. 2023. Chartllama: A multimodal llm for
chart understanding and generation. arXiv preprint
arXiv:2311.16483.
Liu He, Yizhi Song, Hejun Huang, Pinxin Liu, Yunlong
Tang, Daniel Aliaga, and Xin Zhou. 2025. Kubrick:
Multimodal agent collaborations for synthetic video
generation. Preprint, arXiv:2408.10453.
Yi He, Ke Xu, Shixiong Cao, Yang Shi, Qing Chen,
and Nan Cao. 2024. Leveraging foundation models
for crafting narrative visualization: A survey. IEEE
transactions on visualization and computer graphics.
9
Jeffrey Heer and Michael Bostock. 2010. Declarative
language design for interactive visualization. IEEE
Transactions on Visualization and Computer Graph-
ics, 16(6):1149–1156.
Matt-Heun Hong and Anamaria Crisan. 2023. Conver-
sational ai threads for visualizing multidimensional
datasets. Preprint, arXiv:2311.05590.
Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran
Hao, Liuyihan Song, Yang Xu, J Yang, JH Liu,
Chenchen Zhang, Linzheng Chai, and 1 others.
2024.
Opencoder: The open cookbook for top-
tier code large language models.
arXiv preprint
arXiv:2411.04905.
Fantine Huot, Reinald Kim Amplayo, Jennimaria Palo-
maki, Alice Shoshana Jakobovits, Elizabeth Clark,
and Mirella Lapata. 2025. Agents’ room: Narrative
generation through multi-step collaboration. In Inter-
national Conference on Learning Representations.
Mohammed Saidul Islam, Md Tahmid Rahman Laskar,
Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty.
2024. DataNarrative: Automated data-driven story-
telling with visualizations and texts. In Proceedings
of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 19253–19286,
Miami, Florida, USA. Association for Computational
Linguistics.
Carlos E Jimenez, John Yang, Alexander Wettig,
Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R
Narasimhan. 2024. SWE-bench: Can language mod-
els resolve real-world github issues? In The Twelfth
International Conference on Learning Representa-
tions.
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon,
Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei
Han. 2025. Search-r1: Training llms to reason and
leverage search engines with reinforcement learning.
arXiv preprint arXiv:2503.09516.
Max Ku, Thomas Chong, Jonathan Leung, Krish
Shah, Alvin Yu, and Wenhu Chen. 2025.
The-
oremexplainagent: Towards multimodal explana-
tions for llm theorem understanding. arXiv preprint
arXiv:2502.19400.
Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and
Wenhu Chen. 2024. VIEScore: Towards explainable
metrics for conditional image synthesis evaluation.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 12268–12290, Bangkok, Thai-
land. Association for Computational Linguistics.
Guozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan
Zheng, Yu Zhang, Chuangxin Ou, Song Wang, and
Chi Harold Liu. 2024a.
Visualization generation
with large language models: An evaluation. Preprint,
arXiv:2401.11255.
Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya
Du. 2024b. Mlr-copilot: Autonomous machine learn-
ing research based on large language models agents.
Preprint, arXiv:2408.14033.
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,
Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng
Dou. 2025a. Search-o1: Agentic search-enhanced
large reasoning models. CoRR, abs/2501.05366.
Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yu-
tao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng
Dou. 2025b. Webthinker: Empowering large rea-
soning models with deep research capability. CoRR,
abs/2504.21776.
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian
Zhang, Yutao Zhu, and Zhicheng Dou. 2025c. From
matching to generation: A survey on generative infor-
mation retrieval. ACM Transactions on Information
Systems, 43(3):1–62.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
NLG evaluation using gpt-4 with better human align-
ment. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 2511–2522, Singapore. Association for Com-
putational Linguistics.
Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-
ster, Jeff Clune, and David Ha. 2024. The AI Scien-
tist: Towards fully automated open-ended scientific
discovery. arXiv preprint arXiv:2408.06292.
Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Jun-
wei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue
Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei
Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu
Liu, Jingyang Yuan, Shichang Zhang, and 7 others.
2025. Large language model agent: A survey on
methodology, applications and challenges. Preprint,
arXiv:2503.21460.
Paula Maddigan and Teo Susnjak. 2023. Chat2vis: Fine-
tuning data visualisations using multilingual natural
language text and pre-trained large language models.
Preprint, arXiv:2303.14292.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
and 1 others. 2021.
Webgpt: Browser-assisted
question-answering with human feedback.
arXiv
preprint arXiv:2112.09332.
Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-
vio Savarese, and Yingbo Zhou. 2023a. Codegen2:
Lessons for training llms on programming and natu-
ral languages. ICLR.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2023b. Codegen: An open large language
model for code with multi-turn program synthesis.
ICLR.
10
2025-12-10 16:48:53 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:170 | final_prompt"A cartoon-style scientific illustration on a white background depicting the Multimodal DeepResearcher framework. The image is divided into four main sections, each representing a step in the research method. The first section shows a computer conducting iterative web searches with keywords and analyzing webpages, symbolizing the researching phase. The second section illustrates the exemplar textualization process, with a human expert converting charts into text using the Formal Description of Visualization (FDV) method. The third section displays the planning phase, featuring an outline with hierarchical sections and a visualization style guide. The fourth section represents the final report generation, showing a computer generating a multimodal report with interleaved text and visualizations, using D3 for visualization coding and an actor-critic mechanism for refinement. The experimental results are depicted as a series of charts and graphs, with a multimodal LLM providing feedback. All text and labels are in large, clear, dark fonts to ensure readability." - edit_prompt：None - image_path：None - prompt："A cartoon-style scientific illustration on a white background depicting the Multimodal DeepResearcher framework. The image is divided into four main sections, each representing a step in the research method. The first section shows a computer conducting iterative web searches with keywords and analyzing webpages, symbolizing the researching phase. The second section illustrates the exemplar textualization process, with a human expert converting charts into text using the Formal Description of Visualization (FDV) method. The third section displays the planning phase, featuring an outline with hierarchical sections and a visualization style guide. The fourth section represents the final report generation, showing a computer generating a multimodal report with interleaved text and visualizations, using D3 for visualization coding and an actor-critic mechanism for refinement. The experimental results are depicted as a series of charts and graphs, with a multimodal LLM providing feedback. All text and labels are in large, clear, dark fonts to ensure readability."
2025-12-10 16:49:26 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:298 | [figure_layout_sam] 共生成 4 个布局元素
2025-12-10 16:49:26 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:320 | [figure_mask] MinerU 输出目录: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/mineru_recursive
2025-12-10 16:49:26 | CRITICAL | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:326 | mask detail level : 2 
2025-12-10 16:49:26 | CRITICAL | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:327 | [img_path]: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/fig_1765356533.png
2025-12-10 16:49:26 | CRITICAL | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:328 | [mineru_port]: 8001
2025-12-10 16:49:41 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:337 | mineru_items : [{'type': 'title', 'bbox': [0.1, 0.038, 0.895, 0.095], 'png_path': None, 'text': 'Multimodal DeepResearcher Framework', 'depth': 0}, {'type': 'title', 'bbox': [0.084, 0.19, 0.293, 0.233], 'png_path': None, 'text': '1. Researching', 'depth': 0}, {'type': 'image', 'bbox': [0.09117199999999999, 0.26964, 0.27072399999999996, 0.47824], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image', 'bbox': [0.27559217599999997, 0.23135532, 0.42122043200000003, 0.34908468000000004], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'title', 'bbox': [0.323576, 0.35924, 0.398872, 0.38276000000000004], 'png_path': None, 'text': 'Keywords', 'depth': 1}, {'type': 'image', 'bbox': [0.2838284, 0.38136000000000003, 0.332988, 0.43008], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'title', 'bbox': [0.34348599999999996, 0.4032, 0.419868, 0.42672], 'png_path': None, 'text': 'Webpages', 'depth': 1}, {'type': 'image', 'bbox': [0.30619999999999997, 0.42476, 0.35379938, 0.4717888], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'title', 'bbox': [0.100946, 0.48524, 0.288462, 0.5082], 'png_path': None, 'text': 'Iterative Web Search', 'depth': 1}, {'type': 'title', 'bbox': [0.37136, 0.47964000000000007, 0.44738, 0.50764], 'png_path': None, 'text': 'Analysis', 'depth': 1}, {'type': 'title', 'bbox': [0.536, 0.19, 0.92, 0.233], 'png_path': None, 'text': '2. Exemplar Textualization', 'depth': 0}, {'type': 'image', 'bbox': [0.537, 0.245, 0.911, 0.503], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'title', 'bbox': [0.08, 0.594, 0.243, 0.637], 'png_path': None, 'text': '3. Planning', 'depth': 0}, {'type': 'image', 'bbox': [0.088, 0.676, 0.43999999999999995, 0.852], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image_caption', 'bbox': [0.243, 0.86, 0.434, 0.895], 'png_path': None, 'text': 'Strategic Design', 'depth': 0}, {'type': 'title', 'bbox': [0.538, 0.594, 0.839, 0.635], 'png_path': None, 'text': '4. Report Generation', 'depth': 0}, {'type': 'image', 'bbox': [0.550138, 0.653992, 0.727768, 0.838], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image', 'bbox': [0.7627, 0.635, 0.907, 0.795], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image', 'bbox': [0.541832, 0.8261919999999999, 0.597528, 0.9211360000000001], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image_caption', 'bbox': [0.607, 0.84, 0.746, 0.86], 'png_path': None, 'text': 'D3 Visualization Coding', 'depth': 0}, {'type': 'image_caption', 'bbox': [0.607, 0.888, 0.744, 0.906], 'png_path': None, 'text': 'Actor-Critic Retirement', 'depth': 0}, {'type': 'image_caption', 'bbox': [0.756, 0.804, 0.915, 0.828], 'png_path': None, 'text': 'Experimental Results', 'depth': 0}, {'type': 'image', 'bbox': [0.761627372, 0.840247752, 0.918, 0.902], 'png_path': None, 'text': None, 'depth': 2}, {'type': 'image_caption', 'bbox': [0.857, 0.902, 0.915, 0.92], 'png_path': None, 'text': 'Feedback', 'depth': 0}]
2025-12-10 16:49:41 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:561 | [figure_mask] fig_mask size = 24, type distribution = {'text': 9, 'image': 15}, title_text=9, icons(raw)=15
2025-12-10 16:49:41 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:569 | [figure_mask] 共解析出 24 个元素 (via MinerU HTTP + SAM fallback, pixel bbox + raw icons)
2025-12-10 16:49:42 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_2_bg_removed.png
2025-12-10 16:49:43 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_3_bg_removed.png
2025-12-10 16:49:43 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_5_bg_removed.png
2025-12-10 16:49:44 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_7_bg_removed.png
2025-12-10 16:49:45 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_11_bg_removed.png
2025-12-10 16:49:45 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_13_bg_removed.png
2025-12-10 16:49:46 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_14_bg_removed.png
2025-12-10 16:49:47 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_16_bg_removed.png
2025-12-10 16:49:47 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_17_bg_removed.png
2025-12-10 16:49:48 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_18_bg_removed.png
2025-12-10 16:49:49 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_19_bg_removed.png
2025-12-10 16:49:49 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_20_bg_removed.png
2025-12-10 16:49:50 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_21_bg_removed.png
2025-12-10 16:49:50 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_22_bg_removed.png
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:592 | [figure_icon_bg_remover] background removed: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/icons/blk_23_bg_removed.png
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:595 | [figure_icon_bg_remover] processed image/table elements: 15
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [528, 583, 960, 958]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=5.50, top=6.07, width=4.50, height=3.91
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_0.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [527, 170, 960, 543]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=5.49, top=1.77, width=4.51, height=3.89
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_1.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [58, 170, 491, 543]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=0.60, top=1.77, width=4.51, height=3.89
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_2.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [69, 589, 480, 948]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=0.72, top=6.14, width=4.28, height=3.74
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_3.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [528, 583, 960, 958]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=5.50, top=6.07, width=4.50, height=3.91
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_0.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [527, 170, 960, 543]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=5.49, top=1.77, width=4.51, height=3.89
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_1.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [58, 170, 491, 543]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=0.60, top=1.77, width=4.51, height=3.89
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_2.emf
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:659 | [figure_ppt_generation] 添加 EMF：
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:660 |   bbox 像素: [69, 589, 480, 948]
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:661 |   英寸坐标: left=0.72, top=6.14, width=4.28, height=3.74
2025-12-10 16:49:51 | INFO     | dataflow_agent.workflow.wf_paper2figure_with_sam | wf_paper2figure_with_sam.py:662 |   emf_path: /home/ubuntu/liuzhou/myproj/dev/DataFlow-Agent/outputs/paper2figure/1765356519/layout_items/layout_3.emf
