import base64
from typing import Any, Dict, List
from dataflow_agent.state import MainState
from langchain_core.messages import AIMessage, BaseMessage
import aiohttp

from dataflow_agent.llm_callers.base import BaseLLMCaller
from dataflow_agent.logger import get_logger

log = get_logger(__name__)

class VisionLLMCaller(BaseLLMCaller):
    """è§†è§‰LLMè°ƒç”¨å™¨ - æ”¯æŒå›¾åƒè¾“å…¥/è¾“å‡º"""
    
    def __init__(self, 
                 state: MainState,
                 vlm_config: Dict[str, Any],
                 **kwargs):
        """
        Args:
            vlm_config: VLMé…ç½®ï¼ŒåŒ…å«ï¼š
                - mode: "generation" | "edit" | "understanding"
                - input_image: è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆedit/understandingæ¨¡å¼ï¼‰
                - output_image: è¾“å‡ºå›¾åƒä¿å­˜è·¯å¾„ï¼ˆgeneration/editæ¨¡å¼ï¼‰
                - response_format: "image" | "text" (é»˜è®¤æ ¹æ®modeè‡ªåŠ¨åˆ¤æ–­)
        """
        super().__init__(state, **kwargs)
        self.vlm_config = vlm_config
        self.mode = vlm_config.get("mode", "understanding")
    
    async def call(self, messages: List[BaseMessage], bind_post_tools: bool = False) -> AIMessage:
        """è°ƒç”¨VLM"""
        log.info(f"VisionLLMè°ƒç”¨ï¼Œæ¨¡å‹: {self.model_name}, æ¨¡å¼: {self.mode}")
        
        if self.mode in ["generation", "edit"]:
            return await self._call_image_output(messages)
        else:
            return await self._call_image_understanding(messages)
    
    async def _call_image_understanding(self, messages: List[BaseMessage]) -> AIMessage:
        """å›¾åƒç†è§£æ¨¡å¼ - è¾“å…¥å›¾åƒï¼Œè¾“å‡ºæ–‡æœ¬"""
        # è¿™ä¸ªè¿˜æœ‰bugï¼ï¼ï¼


        import httpx
        
        # æ„å»ºåŒ…å«å›¾åƒçš„æ¶ˆæ¯
        processed_messages = []
        for msg in messages:
            if hasattr(msg, 'content') and isinstance(msg.content, str):
                processed_messages.append({
                    "role": msg.type if hasattr(msg, 'type') else "user",
                    "content": msg.content
                })
        
        # å¦‚æœé…ç½®äº†è¾“å…¥å›¾åƒï¼Œæ·»åŠ åˆ°æœ€åä¸€æ¡æ¶ˆæ¯
        if "input_image" in self.vlm_config:
            b64, fmt = self._encode_image(self.vlm_config["input_image"])
            
            # ä¿®æ”¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸ºå¤šæ¨¡æ€æ ¼å¼
            last_msg = processed_messages[-1]
            if last_msg["role"] == "user":
                last_msg["content"] = [
                    {"type": "text", "text": last_msg["content"]},
                    {"type": "image_url", 
                     "image_url": {"url": f"data:image/{fmt};base64,{b64}"}}
                ]
        
        # è°ƒç”¨API
        payload = {
            "model": self.model_name,
            "messages": processed_messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        response_data = await self._post_chat_completions(payload)
        content = response_data["choices"][0]["message"]["content"]
        
        return AIMessage(content=content)
    
    async def _call_image_output(self, messages: List[BaseMessage]) -> AIMessage:
        """å›¾åƒç”Ÿæˆ/ç¼–è¾‘æ¨¡å¼ - è¾“å‡ºå›¾åƒ"""
        from dataflow_agent.toolkits.imtool.req_img import generate_or_edit_and_save_image_async
        
        # æå–promptï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
        prompt = ""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                prompt = msg.content
                break
        
        # è°ƒç”¨å›¾åƒç”Ÿæˆå‡½æ•°
        save_path = self.vlm_config.get("output_image", "./generated_image.png")
        image_path = self.vlm_config.get("input_image") if self.mode == "edit" else None
        
        b64 = await generate_or_edit_and_save_image_async(
            prompt=prompt,
            save_path=save_path,
            api_url=self.state.request.chat_api_url,
            api_key=self.state.request.api_key,
            model=self.model_name,
            image_path=image_path,
            use_edit=(self.mode == "edit"),
            timeout=self.vlm_config.get("timeout", 120),
        )
        
        # è¿”å›å›¾åƒè·¯å¾„ä½œä¸ºå†…å®¹
        content = f"å›¾åƒå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³: {save_path}"
        
        return AIMessage(content=content, additional_kwargs={
            "image_path": save_path,
            "image_base64": b64,
        })
    
    def _encode_image(self, image_path: str) -> tuple:
        """ç¼–ç å›¾åƒä¸ºbase64"""
        with open(image_path, "rb") as f:
            raw = f.read()
        b64 = base64.b64encode(raw).decode("utf-8")
        
        ext = image_path.rsplit(".", 1)[-1].lower()
        if ext in {"jpg", "jpeg"}:
            fmt = "jpeg"
        elif ext == "png":
            fmt = "png"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {ext}")
        
        return b64, fmt
    
    async def _post_chat_completions(self, payload: dict) -> dict:
        """è°ƒç”¨chat completions API"""
        import httpx
        
        url = f"{self.state.request.chat_api_url}/chat/completions".rstrip("/")
        headers = {
            "Authorization": f"Bearer {self.state.request.api_key}",
            "Content-Type": "application/json",
        }
        
        timeout = self.vlm_config.get("timeout", 120)
        async with httpx.AsyncClient(timeout=httpx.Timeout(timeout)) as client:
            resp = await client.post(url, headers=headers, json=payload)
            resp.raise_for_status()
            return resp.json()
        

# ======================================================================
# å¿«é€Ÿè‡ªæµ‹ï¼špython vision.py <image_path>
# ======================================================================
if __name__ == "__main__":
    """
    ç”¨æ³•:
        python vision.py /path/to/your/image.png
    """
    import os
    import sys
    import asyncio
    from types import SimpleNamespace
    from pathlib import Path
    from langchain_core.messages import HumanMessage

    async def _quick_test(img_path: str):
        # 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
        api_url = os.getenv("DF_API_URL")
        api_key = os.getenv("DF_API_KEY")
        if not api_url or not api_key:
            print("âŒ  è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ DF_API_URL / DF_API_KEY")
            sys.exit(1)

        # 2. æ£€æŸ¥å›¾ç‰‡
        img_path = Path(img_path).expanduser().resolve()
        if not img_path.exists():
            print(f"âŒ  å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
            sys.exit(1)

        # 3. æ„é€ æç®€ MainState
        request = SimpleNamespace(chat_api_url=api_url.rstrip("/"), api_key=api_key, model = "gemini-2.5-flash-image-preview")
        state = SimpleNamespace(request=request)

        # 4. å®ä¾‹åŒ–å¹¶è°ƒç”¨
        caller = VisionLLMCaller(
            state=state,
            vlm_config={
                "mode": "understanding",
                "input_image": str(img_path),
                "timeout": 60,
            }
        )
        print("ğŸš€ æ­£åœ¨è¯·æ±‚æ¨¡å‹ï¼Œè¯·ç¨å€™ â€¦")
        ai_msg = await caller.call([HumanMessage(content="æè¿°è¿™ä¸ªimg!")])

        print("\n================  ç»“æœ  ================")
        print(ai_msg.content)
        print("========================================")

    # -------- å…¥å£ --------
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python vision.py <image_path>")
        sys.exit(0)

    asyncio.run(_quick_test(sys.argv[1]))